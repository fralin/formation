{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09ff3cc4",
   "metadata": {},
   "source": [
    "# 11 - Séparer les classes avec des lignes de séparation\n",
    "\n",
    "\n",
    "Nous allons développer un réseau neuronal simple dans ce chapitre de notre tutoriel. Un réseau capable de séparer deux classes, qui sont séparables par une ligne droite dans un espace de caractéristiques à 2 dimensions.\n",
    "\n",
    "## Séparation par ligne\n",
    "\n",
    "Avant de commencer à programmer un réseau neuronal simple, nous allons développer un concept différent. Nous voulons rechercher les lignes droites qui séparent deux points ou deux classes dans un plan. Nous n'étudierons que les lignes droites passant par l'origine. Nous étudierons les lignes droites générales plus tard dans le tutoriel.\n",
    "\n",
    "Vous pouvez imaginer que vous avez deux attributs décrivant un objet comestible comme un fruit par exemple : \"douceur\" et \"aigreur\".\n",
    "\n",
    "Nous pourrions les décrire par des points dans un espace bidimensionnel. L'axe A est utilisé pour les valeurs du goût sucré et l'axe y est utilisé de manière correspondante pour les valeurs de l'acidité. Imaginons maintenant que nous ayons deux fruits comme points dans cet espace, c'est-à-dire une orange à la position (3,5, 1,8) et un citron à la position (1,1, 3,9).\n",
    "\n",
    "Nous pourrions définir des lignes de séparation pour définir les points qui ressemblent davantage à des citrons et ceux qui ressemblent davantage à des oranges.\n",
    "\n",
    "Dans le diagramme suivant, nous représentons un citron et une orange. La ligne verte sépare les deux points. Nous supposons que tous les autres citrons sont au-dessus de cette ligne et que toutes les oranges sont en dessous de cette ligne.\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/illustration11_1.png\" width=\"50%\">\n",
    "</center>    \n",
    "\n",
    "La ligne verte est définie par:\n",
    "$$y=mx$$\n",
    "\n",
    "où :\n",
    "\n",
    "$m$ est la pente ou le gradient de la ligne et $x$ est la variable indépendante de la fonction.\n",
    "$$m=\\frac{p_1}{p_2}x$$\n",
    "\n",
    "Cela signifie qu'un point $P'=(p'_1, p'_2)$ est sur cette ligne, si la condition suivante est remplie :\n",
    "$$mp'_1-p'_2=0$$\n",
    "\n",
    "Le programme Python suivant trace un graphique décrivant la situation décrite précédemment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6100d4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "X = np.arange(0, 7)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(3.5, 1.8, \"o\", \n",
    "        color=\"darkorange\", \n",
    "        markersize=15)\n",
    "ax.plot(1.1, 3.9, \"oy\", \n",
    "        markersize=15)\n",
    "\n",
    "point_on_line = (4, 4.5)\n",
    "# calculate gradient:\n",
    "m = point_on_line[1] / point_on_line[0]  \n",
    "ax.plot(X, m * X, \"g-\", linewidth=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b01893d",
   "metadata": {},
   "source": [
    "Il est clair qu'un point $1=(a_1,a_2)$ n'est pas sur la ligne, si $a_1\\cdot m-a_2$ n'est pas égal à 0. Nous voulons en savoir plus. Nous voulons savoir si un point est au-dessus ou au-dessous d'une ligne droite.\n",
    " \n",
    "<center>\n",
    "    <img src=\"img/illustration11_2.png\" width=\"50%\">\n",
    "</center>\n",
    "\n",
    "Si un point $B=(b_1,b_2)$ est en dessous de cette ligne, il doit y avoir un $\\delta_B>0$ de sorte que le point $(b_1,b_2+\\delta_B)$\n",
    "\n",
    "Cela signifie que\n",
    "\n",
    "$$m\\cdot b_1-(b_2+\\delta_B)=0$$\n",
    "\n",
    "ce qui peut être réarrangé en:\n",
    "\n",
    "$$m\\cdot b_1 -b_2 = \\delta_B$$\n",
    "\n",
    "Enfin, nous avons un critère pour qu'un point soit en dessous de la ligne. $m\\cdot b_1-b_2$ est positif, car $\\delta_B$ est positif.\n",
    "\n",
    "Le raisonnement pour \"un point est au-dessus de la ligne\" est analogue : Si un point $A=(a_1, a_2)$ est au-dessus de la ligne, il doit exister un $\\delta_A$ de sorte que le point $(a_1, a_2-\\delta_A)$ soit sur la ligne.\n",
    "\n",
    "Cela signifie que:\n",
    "\n",
    "$$m\\cdot a_1-(a_2-\\delta_A)=0$$\n",
    "\n",
    "ce qui peut être réarrangé en:\n",
    "\n",
    "$$m\\cdot a_1-a_2 = - \\delta_A$$\n",
    "\n",
    "En résumé, nous pouvons dire : Un point $P(p_1, p2)$ se trouve:\n",
    "- en dessous de la ligne droite, si $m\\cdot p_1 - p_2 > 0$\n",
    "- sur la ligne droite, si $m\\cdot p_1 - p_2 =0$\n",
    "- au-dessus de la ligne droite, si $m\\cdot p_1 - p_2 <0$\n",
    "\n",
    "Nous pouvons maintenant le vérifier sur nos fruits. Le citron a les coordonnées (1,1, 3,9) et l'orange les coordonnées 3,5, 1,8. Le point sur la droite, que nous avons utilisé pour définir notre droite de séparation a pour valeurs (4, 4,5). Donc m est le quotient de 4,5 et 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e89ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemon = (1.1, 3.9)\n",
    "orange = (3.5, 1.8)\n",
    "m = 4.5 / 4\n",
    "\n",
    "# check if the orange is below the line,\n",
    "# a positive value is expected:\n",
    "print(orange[0] * m - orange[1])\n",
    "\n",
    "# check if the lemon is above the line,\n",
    "# a negative value is expected:\n",
    "print(lemon[0] * m - lemon[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4aaa9a",
   "metadata": {},
   "source": [
    "Nous n'avons pas calculé la ligne verte à l'aide de formules ou de méthodes mathématiques, mais l'avons déterminée arbitrairement par un jugement visuel. Nous aurions pu choisir d'autres lignes également.\n",
    "\n",
    "Le programme Python suivant calcule et rend un certain nombre de lignes. Toutes passent par l'origine, c'est-à-dire le point (0, 0). Les lignes rouges sont totalement inutilisables pour séparer les deux fruits, car dans ce cas, le citron et l'orange se trouvent du même côté de la ligne droite. Cependant, il est évident que même les verts ne sont pas très utiles si nous avons plus que ces deux fruits. Certains citrons peuvent être plus sucrés et certaines oranges peuvent être assez acides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distance_function(a, b, c):\n",
    "    \"\"\" 0 = ax + by + c \"\"\"\n",
    "    def distance(x, y):\n",
    "        \"\"\" \n",
    "        returns tuple (d, pos)\n",
    "        d is the distance\n",
    "        If pos == -1 point is below the line, \n",
    "        0 on the line and +1 if above the line\n",
    "        \"\"\"\n",
    "        nom = a * x + b * y + c\n",
    "        if nom == 0:\n",
    "            pos = 0\n",
    "        elif (nom<0 and b<0) or (nom>0 and b>0):\n",
    "            pos = -1\n",
    "        else:\n",
    "            pos = 1\n",
    "        return (np.absolute(nom) / np.sqrt( a ** 2 + b ** 2), pos)\n",
    "    return distance\n",
    "    \n",
    "orange = (4.5, 1.8)\n",
    "lemon = (1.1, 3.9)\n",
    "fruits_coords = [orange, lemon]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"sweetness\")\n",
    "ax.set_ylabel(\"sourness\")\n",
    "x_min, x_max = -1, 7\n",
    "y_min, y_max = -1, 8\n",
    "ax.set_xlim([x_min, x_max])\n",
    "ax.set_ylim([y_min, y_max])\n",
    "X = np.arange(x_min, x_max, 0.1)\n",
    "\n",
    "step = 0.05\n",
    "for x in np.arange(0, 1+step, step):\n",
    "    slope = np.tan(np.arccos(x))\n",
    "    dist4line1 = create_distance_function(slope, -1, 0)\n",
    "    Y = slope * X\n",
    "    results = []\n",
    "    for point in fruits_coords:\n",
    "        results.append(dist4line1(*point))\n",
    "    if (results[0][1] != results[1][1]):\n",
    "        ax.plot(X, Y, \"g-\", linewidth=0.8, alpha=0.9)\n",
    "    else:\n",
    "        ax.plot(X, Y, \"r-\", linewidth=0.8, alpha=0.9)\n",
    "\n",
    "size = 10\n",
    "for (index, (x, y)) in enumerate(fruits_coords):\n",
    "    if index== 0:\n",
    "        ax.plot(x, y, \"o\", \n",
    "                color=\"darkorange\", \n",
    "                markersize=size)\n",
    "    else:\n",
    "        ax.plot(x, y, \"oy\", \n",
    "                markersize=size)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf14479",
   "metadata": {},
   "source": [
    "En fait, nous avons effectué une classification sur la base de notre ligne de démarcation. Même si presque personne ne la décrirait comme telle.\n",
    "\n",
    "Il est facile d'imaginer que nous avons plusieurs citrons et oranges avec des valeurs d'acidité et de douceur légèrement différentes. Cela signifie que nous avons une classe de citrons ```classe1``` et une classe d'oranges ```classe2```. Ceci est illustré dans le diagramme suivant.\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/illustration11_4.png\" width=\"40%\">\n",
    "</center>\n",
    "\n",
    "Nous allons faire \"pousser\" des oranges et des citrons avec un programme Python. Nous allons créer ces deux classes en créant de manière aléatoire des points à l'intérieur d'un cercle dont le point central et le rayon sont définis. Le code Python suivant va créer les classes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0203e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def points_within_circle(radius, \n",
    "                         center=(0, 0),\n",
    "                         number_of_points=100):\n",
    "    center_x, center_y = center\n",
    "    r = radius * np.sqrt(np.random.random((number_of_points,)))\n",
    "    theta = np.random.random((number_of_points,)) * 2 * np.pi\n",
    "    x = center_x + r * np.cos(theta)\n",
    "    y = center_y + r * np.sin(theta)\n",
    "    return x, y\n",
    "\n",
    "X = np.arange(0, 8)\n",
    "fig, ax = plt.subplots()\n",
    "oranges_x, oranges_y = points_within_circle(1.6, (5, 2), 100)\n",
    "lemons_x, lemons_y = points_within_circle(1.9, (2, 5), 100)\n",
    "\n",
    "ax.scatter(oranges_x, \n",
    "           oranges_y, \n",
    "           c=\"orange\", \n",
    "           label=\"oranges\")\n",
    "ax.scatter(lemons_x, \n",
    "           lemons_y, \n",
    "           c=\"y\", \n",
    "           label=\"lemons\")\n",
    "\n",
    "ax.plot(X, 0.9 * X, \"g-\", linewidth=2)\n",
    "\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbc61aa",
   "metadata": {},
   "source": [
    "## Détermination automatique de la ligne de démarcation\n",
    "\n",
    "La ligne de séparation a de nouveau été fixée arbitrairement à l'œil. La question se pose de savoir comment le faire systématiquement. Nous ne considérons toujours que des lignes droites passant par l'origine, qui sont définies uniquement par leur pente.\n",
    "\n",
    "Reprenons le cas le plus simple, avec un citron et une orange. Nous commençons par une ligne arbitraire qui ne sépare absolument pas notre orange et notre citron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2443dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_fruits(p1, p2, point_on_line=(5,1)):\n",
    "    X = np.arange(0, 7)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(p1[0], p1[1], \"o\", \n",
    "            color=\"darkorange\", \n",
    "            markersize=15)\n",
    "    ax.annotate(\"Orange\", \n",
    "                xy=(p1[0], p1[1]), \n",
    "                xytext=(p1[0]+0.5, p1[1]+0.5),\n",
    "                arrowprops=dict(facecolor='orange', shrink=0.05))\n",
    "    ax.plot(p2[0], p2[1], \"o\", \n",
    "            color=\"yellow\", \n",
    "            markersize=15)\n",
    "    ax.annotate(\"Lemon\", \n",
    "                xy=(p2[0], p2[1]), \n",
    "                xytext=(p2[0]-0.5, p2[1]-0.5),\n",
    "                arrowprops=dict(facecolor='orange', shrink=0.05))\n",
    "    ax.plot(*point_on_line, \"x\", \n",
    "            color=\"darkorange\", \n",
    "            markersize=15)\n",
    "    # calculate gradient:\n",
    "    m = point_on_line[1] / point_on_line[0]  \n",
    "    ax.plot(X, m * X, \"g-\", linewidth=3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "orange = (4, 2)\n",
    "lemon = (1, 3)\n",
    "point = (5, 1)\n",
    "plot_fruits(p1=orange, p2=lemon, point_on_line=point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58efbab2",
   "metadata": {},
   "source": [
    "Nous pouvons voir que la ligne ne convient pas comme ligne de séparation, car le citron et l'orange sont tous deux au-dessus de la ligne. Nous pouvons calculer si l'orange est au-dessus ou au-dessous de la ligne, si nous vérifions que ```m * orange[0] + orange[1]``` est supérieur à zéro (\"au-dessous de la ligne\") ou inférieur à zéro (\"au-dessus de la ligne\") :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad4f748",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = point[1] / point[0]\n",
    "m * orange[0] - orange[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dccab83",
   "metadata": {},
   "source": [
    "Cela signifie que l'orange est au-dessus de la ligne, mais qu'elle devrait être en dessous de la ligne. Dans cet exemple, l'idéal serait d'avoir une ligne de séparation juste au-dessus de l'orange. Ainsi, une ligne passant par le point $p_3 = (4, 2+\\delta)$ avec $\\delta$ égal à 0,3 satisfait à la condition :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.3\n",
    "plot_fruits(p1=(4, 2), p2=(1, 3), point_on_line=(4, 2+delta))\n",
    "\n",
    "new_slope = (2 + delta) / 4\n",
    "# position of orange:\n",
    "new_slope * orange[0] - orange[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e98cea",
   "metadata": {},
   "source": [
    "Cela signifie que l'orange est maintenant en dessous de la ligne.\n",
    "\n",
    "Nous pouvons dire que l'erreur entre notre pente initiale et la pente visée est de :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_slope = new_slope\n",
    "initial_slope = point[1] / point[0]\n",
    "error = targeted_slope - initial_slope\n",
    "\n",
    "# the targeted_slope can be seen as the following sum:\n",
    "initial_slope + error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60961b21",
   "metadata": {},
   "source": [
    "Nous allons maintenant examiner ce qui se passe si nous appliquons ce mécanisme de correction à d'autres fruits. Pour créer les clusters de citrons et d'oranges, nous allons utiliser cette fois la fonction ```make_blobs``` de ```sklearn.datasets```. Le centre des oranges est fixé à (1, 1,5) et celui des citrons à (1,5, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee063827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "number_of_samples = 9\n",
    "centers = [(1, 1.5), (1.5, 1)]\n",
    "data, labels = make_blobs(n_samples=number_of_samples, \n",
    "                          cluster_std=0.2,\n",
    "                          centers=np.array(centers),\n",
    "                          random_state=42)\n",
    "\n",
    "fruits = [(data[i], labels[i]) for i in range(len(data))]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "colours = [\"yellow\", \"orange\"]\n",
    "label_name = [\"Lemons\", \"Oranges\"]\n",
    "for label in range(0, 2):\n",
    "    ax.scatter(data[labels==label, 0], \n",
    "               data[labels==label, 1], \n",
    "               c=colours[label], \n",
    "               s=40, \n",
    "               label=label_name[label])\n",
    "\n",
    "ax.set(xlabel='X', ylabel='Y', title='fruits');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd41b92",
   "metadata": {},
   "source": [
    "Nous allons appliquer l'idée développée précédemment de corriger l'erreur sur ces données. Nous allons itérer sur les fruits et corriger l'erreur de la manière que nous avons démontrée juste avant : Chaque fois qu'un fruit se trouve du mauvais côté de la ligne droite, nous réinitialiserons la pente en conséquence :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e8ef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = 0.3\n",
    "def adjust(slope=0.3):\n",
    "    line = None\n",
    "    delta = 0.1\n",
    "    counter = -1\n",
    "    for ((x, y), label) in zip(data, labels):   \n",
    "        counter += 1\n",
    "        ax.scatter(x, y,\n",
    "                   color=\"y\" if label == 0 else \"orange\")\n",
    "        ax.annotate(str(counter), \n",
    "                    (x, y))\n",
    " \n",
    "        pos2line = slope * x - y\n",
    "        target_slope = (y + delta) / x\n",
    "        error = (target_slope - slope) \n",
    "        #print(label, pos2line)\n",
    "        if label == 1 and pos2line < 0:\n",
    "            # point is above line but should be below \n",
    "            # => increment slope\n",
    "            print(slope, error)\n",
    "            slope += error \n",
    "            print(slope, x, y)\n",
    "            ax.plot(X, slope * X, \n",
    "                    linewidth=2, label=str(counter))\n",
    "\n",
    "        elif label == 0 and pos2line > 0:\n",
    "            # point is below line but should be above \n",
    "            # => decrement slope\n",
    "            #print(pos2line, label)\n",
    "            print(slope, error)\n",
    "            slope += error \n",
    "            print(slope, x, y)\n",
    "            ax.plot(X, slope * X,  \n",
    "                    linewidth=2, label=str(counter))\n",
    "    return slope\n",
    "\n",
    "X = np.arange(0, 3)\n",
    "fig, ax = plt.subplots()\n",
    "colours = [\"orange\", \"yellow\"]\n",
    "label_name = [\"Oranges\", \"Lemons\"]\n",
    "\n",
    "\n",
    "ax.set(xlabel='X', ylabel='Y', title='fruits')\n",
    "slope_count = 1\n",
    "ax.plot(X, \n",
    "        slope * X,  \n",
    "        linewidth=2,\n",
    "        label=\"initial\")\n",
    "slope = adjust(slope)\n",
    "\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "print(f'The final value for the slope: {slope}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78a8ee9",
   "metadata": {},
   "source": [
    "Cela a bien fonctionné, mais ce n'est toujours pas bon. Pour montrer ce qui peut mal se passer, nous allons ajouter une orange dans la zone où les citrons sont censés être :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bb2c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((data, np.array([[1.1, 1.6]])))\n",
    "labels = np.concatenate((labels, np.array([1])))\n",
    "fig, ax = plt.subplots()\n",
    "colours = [\"yellow\", \"orange\"]\n",
    "label_name = [\"Lemons\", \"Oranges\"]\n",
    "for label in range(0, 2):\n",
    "    ax.scatter(data[labels==label, 0], \n",
    "               data[labels==label, 1], \n",
    "               c=colours[label], \n",
    "               s=40, \n",
    "               label=label_name[label])\n",
    "\n",
    "ax.set(xlabel='X', ylabel='Y', title='fruits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a938c",
   "metadata": {},
   "source": [
    "Nous allons appliquer notre algorithme adaptatif à ce jeu de données étendu :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1555618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_slope = 0.3\n",
    "def adjust(slope=0.3):\n",
    "    line = None\n",
    "    delta = 0.1\n",
    "    counter = -1\n",
    "    for ((x, y), label) in zip(data, labels):   \n",
    "        counter += 1   \n",
    "        ax.scatter(x, y,\n",
    "                   color=\"y\" if label == 0 else \"orange\")\n",
    "        ax.annotate(str(counter), \n",
    "                    (x, y))\n",
    " \n",
    "        pos2line = slope * x - y\n",
    "        target_slope = (y + delta) / x\n",
    "        error = (target_slope - slope) \n",
    "        #print(label, pos2line)\n",
    "        if label == 1 and pos2line < 0:\n",
    "            # point is above line but should be below \n",
    "            # => increment slope\n",
    "            print(slope, error)\n",
    "            slope += error \n",
    "            ax.plot(X, slope * X, \n",
    "                    linewidth=2, label=str(counter))\n",
    "\n",
    "        elif label == 0 and pos2line > 0:\n",
    "            # point is below line but should be above \n",
    "            # => decrement slope\n",
    "            print(slope, error)\n",
    "            slope += error \n",
    "            ax.plot(X, slope * X,  \n",
    "                    linewidth=2, label=str(counter))\n",
    "    return slope\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "colours = [\"orange\", \"yellow\"]\n",
    "label_name = [\"Oranges\", \"Lemons\"]\n",
    "\n",
    "\n",
    "ax.set(xlabel='X', ylabel='Y', title='fruits')\n",
    "slope_count = 1\n",
    "ax.plot(X, \n",
    "        start_slope * X,  \n",
    "        linewidth=2,\n",
    "        label=\"initial\")\n",
    "slope = adjust(start_slope)\n",
    "\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "print(f'The final value for the slope: {slope}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8295425c",
   "metadata": {},
   "source": [
    "Nous pouvons voir que l'orange nouvellement ajouté \"détruit\" le résultat précédemment créé. La ligne numéro 3 (verte) était parfaite. La ligne orange qui est positionnée à l'intérieur des citrons a provoqué la création de la ligne rouge.\n",
    "\n",
    "Au lieu de corriger complètement l'erreur, nous devrions seulement la corriger un peu dans la direction nécessaire. De cette façon, les valeurs aberrantes ne seront pas capables de changer complètement le résultat. Pour ce faire, nous allons introduire un taux d'apprentissage. Nous utiliserons le taux d'apprentissage pour modifier les corrections, c'est-à-dire les rendre moins importantes.\n",
    "\n",
    "Le programme Python suivant calcule une ligne de division en passant par tous les fruits et ajuste dynamiquement la pente de la ligne de division que nous voulons calculer. Si un point est au-dessus de la ligne mais devrait être en dessous de la ligne, la pente sera incrémentée par la valeur du learning_rate multipliée par l'erreur absolue. Si le point est au-dessous de la ligne mais devrait être au-dessus de la ligne, la pente sera décrémentée de la valeur de learning_rate multipliée par l'erreur absolue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98aee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate, start_slope = 0.1, 0.3\n",
    "def adjust(slope=0.3, learning_rate=0.3):\n",
    "    line = None\n",
    "    delta = 0.3\n",
    "    counter = -1\n",
    "    for ((x, y), label) in zip(data, labels):   \n",
    "        counter += 1 \n",
    "        ax.scatter(x, y,\n",
    "                   color=\"y\" if label == 0 else \"orange\")\n",
    "        ax.annotate(str(counter), \n",
    "                    (x, y))\n",
    " \n",
    "        pos2line = slope * x - y\n",
    "        target_slope = (y + delta) / x\n",
    "        error = (target_slope - slope) \n",
    "        if label == 1 and pos2line < 0:\n",
    "            # point is above line but should be below \n",
    "            # => increment slope\n",
    "            slope += error * learning_rate\n",
    "            ax.plot(X, slope * X, \n",
    "                    linewidth=2, label=str(counter))\n",
    "\n",
    "        elif label == 0 and pos2line > 0:\n",
    "            # point is below line but should be above \n",
    "            # => decrement slope\n",
    "            slope += error * learning_rate\n",
    "            ax.plot(X, slope * X,  \n",
    "                    linewidth=2, label=str(counter))\n",
    "    return slope\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "colours = [\"orange\", \"yellow\"]\n",
    "label_name = [\"Oranges\", \"Lemons\"]\n",
    "\n",
    "\n",
    "ax.set(xlabel='X', ylabel='Y', title='fruits')\n",
    "slope_count = 1\n",
    "ax.plot(X, \n",
    "        start_slope * X,  \n",
    "        linewidth=2,\n",
    "        label=\"initial\")\n",
    "slope = adjust(start_slope, learning_rate)\n",
    "\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "print(slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6855a4",
   "metadata": {},
   "source": [
    "Nous pouvons voir dans la classe précédente que nous n'avons pas trouvé une ligne de démarcation correcte. La raison en est que notre taux d'apprentissage était trop faible pour l'ensemble des données, c'est-à-dire que nous n'avons pas assez de fruits pour un taux d'apprentissage aussi faible. Nous pouvons essayer d'obtenir plus de fruits ou nous pouvons appeler adjust plusieurs fois, c'est-à-dire répéter l'apprentissage avec les mêmes données. Nous faisons cela dans le code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c752f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "colours = [\"orange\", \"yellow\"]\n",
    "label_name = [\"Oranges\", \"Lemons\"]\n",
    "\n",
    "\n",
    "ax.set(xlabel='X', ylabel='Y', title='fruits')\n",
    "slope_count = 1\n",
    "ax.plot(X, \n",
    "        start_slope * X,  \n",
    "        linewidth=2,\n",
    "        label=\"initial\")\n",
    "slope = adjust(start_slope, learning_rate)\n",
    "# redo the learning, we use the current slope as the start slope:\n",
    "slope = adjust(slope, learning_rate)\n",
    "# and again once more:\n",
    "slope = adjust(slope, learning_rate)\n",
    "\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "print(slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d3cf7b",
   "metadata": {},
   "source": [
    "Nous pouvons être satisfaits maintenant !\n",
    "\n",
    "Dans le chapitre suivant, nous verrons que cette idée peut s'appliquer à des réseaux neuronaux simples ne comportant qu'un seul neurone.\n",
    "\n",
    "## Un réseau neuronal simple\n",
    "\n",
    "Nous avons été capables de séparer les deux classes avec une ligne droite. On peut se demander quel est le rapport avec les réseaux neuronaux. Nous allons établir ce lien ci-dessous.\n",
    "\n",
    "Nous allons définir un réseau neuronal pour classer les ensembles de données précédents. Notre réseau neuronal sera composé d'un seul neurone. Un neurone avec deux valeurs d'entrée, l'une pour l'\"aigreur\" et l'autre pour la \"douceur\".\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/illustration11_5.png\" width=\"50%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e9532e",
   "metadata": {},
   "source": [
    "Les deux valeurs d'entrée - appelées in_data dans notre programme Python ci-dessous - doivent être pondérées par des valeurs de poids. Pour résoudre notre problème, nous définissons une classe Perceptron. Une instance de cette classe est un Perceptron (ou Neuron). Elle peut être initialisée avec la longueur d'entrée, c'est-à-dire le nombre de valeurs d'entrée, et les poids, qui peuvent être donnés sous forme de liste, de tuple ou de tableau. Si aucune valeur n'est donnée pour les poids ou si le paramètre est défini sur None, nous initialiserons les poids à 1 / input_length.\n",
    "\n",
    "Dans l'exemple suivant, nous choisissons -0.45 et 0.5 comme valeurs pour les poids. Ce n'est pas la façon normale de procéder. Un réseau neuronal calcule les poids automatiquement pendant sa phase de formation, comme nous l'apprendrons plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba5216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, weights):\n",
    "        \"\"\"\n",
    "        'weights' can be a numpy array, list or a tuple with the\n",
    "        actual values of the weights. The number of input values\n",
    "        is indirectly defined by the length of 'weights'\n",
    "        \"\"\"\n",
    "        self.weights = np.array(weights)\n",
    "    \n",
    "    def __call__(self, in_data):\n",
    "        weighted_input = self.weights * in_data\n",
    "        weighted_sum = weighted_input.sum()\n",
    "        return weighted_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee5fe31",
   "metadata": {},
   "source": [
    "Chaque instance de ce Perceptron est appelable comme une fonction avec une liste (ou un tableau) de deux éléments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cda66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Perceptron(weights=[-0.45, 0.5])\n",
    "p([2.9, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a463b3",
   "metadata": {},
   "source": [
    "Nous pouvons l'appeler avec les données de nos citrons et oranges :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0993116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for point in zip(oranges_x[:10], oranges_y[:10]):\n",
    "    res = p(point)\n",
    "    print(res, end=\", \")\n",
    "\n",
    "for point in zip(lemons_x[:10], lemons_y[:10]):\n",
    "    res = p(point)\n",
    "    print(res, end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbec04c",
   "metadata": {},
   "source": [
    "Nous pouvons voir que nous obtenons une valeur négative si nous saisissons une orange et une valeur positive si nous saisissons un citron. Avec ces connaissances, nous pouvons calculer la précision de notre réseau neuronal sur cet ensemble de données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5dc994",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "evaluation = Counter()\n",
    "for point in zip(oranges_x, oranges_y):\n",
    "    res = p(point)\n",
    "    if res < 0:\n",
    "        evaluation['corrects'] += 1\n",
    "    else:\n",
    "        evaluation['wrongs'] += 1\n",
    "\n",
    "\n",
    "for point in zip(lemons_x, lemons_y):\n",
    "    res = p(point)\n",
    "    if res >= 0:\n",
    "        evaluation['corrects'] += 1\n",
    "    else:\n",
    "        evaluation['wrongs'] += 1\n",
    "\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfd3218",
   "metadata": {},
   "source": [
    "Comment fonctionne le calcul ? Nous multiplions les valeurs d'entrée avec les poids et obtenons des valeurs négatives et positives. Examinons ce que nous obtenons, si le calcul donne 0 :\n",
    "\n",
    "$$\\omega_1\\cdot x_1+\\omega_2\\cdot x_2 = 0$$\n",
    "\n",
    "Nous pouvons transformer cette équation en:\n",
    "$$x_2=\\frac{\\omega_1}{\\omega_2}\\cdot x_1$$\n",
    "\n",
    "Nous pouvons comparer cela avec la forme générale d'une ligne droite:\n",
    "\n",
    "$$y=m\\cdot x +c$$\n",
    "\n",
    "où :\n",
    "\n",
    "- $m$ est la pente ou le gradient de la ligne.\n",
    "- $c$ est l'ordonnée à l'origine de la droite.\n",
    "- $x$ est la variable indépendante de la fonction.\n",
    "\n",
    "Nous pouvons facilement voir que notre équation correspond à la définition d'une ligne et de la pente (alias gradient) $m=\\frac{\\omega_1}{\\omega_2}$ et $c$ est égal à 0.\n",
    "\n",
    "Il s'agit d'une ligne droite séparant les oranges et les citrons, que l'on appelle la ```frontière de décision```.\n",
    "\n",
    "Nous visualisons cela avec le programme Python suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdf6835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "slope = 0.1\n",
    "\n",
    "X = np.arange(0, 8)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(oranges_x, \n",
    "           oranges_y, \n",
    "           c=\"orange\", \n",
    "           label=\"oranges\")\n",
    "ax.scatter(lemons_x, \n",
    "           lemons_y, \n",
    "           c=\"y\", \n",
    "           label=\"lemons\")\n",
    "\n",
    "slope = 0.45 / 0.5\n",
    "ax.plot(X, slope * X,  linewidth=2)\n",
    "\n",
    "\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "print(slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac7a0b",
   "metadata": {},
   "source": [
    "## Formation d'un réseau neuronal\n",
    "\n",
    "Comme nous l'avons mentionné dans la section précédente : Nous n'avons pas entraîné notre réseau. Nous avons ajusté les poids à des valeurs dont nous savons qu'elles formeraient une ligne de démarcation. Nous souhaitons maintenant démontrer ce qui est nécessaire pour former notre réseau neuronal simple.\n",
    "\n",
    "Avant de commencer cette tâche, nous allons séparer nos données en données d'apprentissage et de test dans le programme Python suivant. En attribuant la valeur 42 au paramètre ```random_state```, nous obtiendrons le même résultat à chaque exécution, ce qui peut être utile pour le débogage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3947ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "oranges = list(zip(oranges_x, oranges_y))\n",
    "lemons = list(zip(lemons_x, lemons_y))\n",
    "\n",
    "# labelling oranges with 0 and lemons with 1:\n",
    "labelled_data = list(zip(oranges + lemons, \n",
    "                         [0] * len(oranges) + [1] * len(lemons)))\n",
    "random.shuffle(labelled_data)\n",
    "\n",
    "data, labels = zip(*labelled_data)\n",
    "\n",
    "res = train_test_split(data, labels, \n",
    "                       train_size=0.8,\n",
    "                       test_size=0.2,\n",
    "                       random_state=42)\n",
    "train_data, test_data, train_labels, test_labels = res    \n",
    "print(train_data[:10], train_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfff3f0",
   "metadata": {},
   "source": [
    "Comme nous commençons avec deux poids arbitraires, nous ne pouvons pas nous attendre à ce que le résultat soit correct. Pour certains points (fruits), il peut retourner la bonne valeur, c'est-à-dire 1 pour un citron et 0 pour une orange. Dans le cas où nous obtenons un résultat erroné, nous devons corriger nos valeurs de poids. Tout d'abord, nous devons calculer l'erreur. L'erreur est la différence entre la valeur cible ou attendue ```target_result``` et la valeur calculée ```calculated_result```. Avec cette erreur, nous devons ajuster les valeurs de poids avec une valeur incrémentale, c'est-à-dire:\n",
    "\n",
    "$$\\omega_1 = \\omega_1 + \\Delta\\omega_1\\text{ et }\\omega_2 = \\omega_2 + \\Delta\\omega_2$$\n",
    "\n",
    "<center>\n",
    "    <img src=\"img\\illustration11_6.png\" width=\"50%\">\n",
    "</center>\n",
    "\n",
    "Si l'erreur $e$ est égale à 0, c'est-à-dire que le résultat cible est égal au résultat calculé, nous n'avons rien à faire. Le réseau est parfait pour ces valeurs d'entrée. Si l'erreur n'est pas égale, nous devons modifier les poids. Nous devons modifier les poids en leur ajoutant de petites valeurs. Ces valeurs peuvent être positives ou négatives. La quantité de poids que nous devons modifier dépend de l'erreur et de la valeur d'entrée. Supposons, $x_1=0$ et $x_2>0$. Dans ce cas le résultat dans ce cas résulte uniquement sur l'entrée $x_2$. Cela signifie d'autre part que nous pouvons minimiser l'erreur en modifiant uniquement $\\omega_2$. Si l'erreur est négative, nous devrons lui ajouter une valeur négative, et si l'erreur est positive, nous devrons lui ajouter une valeur positive. A partir de là, nous pouvons comprendre que quelles que soient les valeurs d'entrée, nous pouvons les multiplier par l'erreur et nous obtenons des valeurs, que nous pouvons ajouter aux poids. Il manque encore une chose : En faisant ça, on apprendrait à être rapide. Nous avons beaucoup d'échantillons et chaque échantillon ne devrait changer les poids qu'un tout petit peu. Nous devons donc multiplier ce résultat par un taux d'apprentissage ```self.learning_rate```. Le taux d'apprentissage est utilisé pour contrôler la vitesse à laquelle les poids sont mis à jour. De petites valeurs pour le taux d'apprentissage entraînent un long processus d'apprentissage, de plus grandes valeurs comportent le risque de se retrouver avec des valeurs de poids sous-optimales. Nous verrons cela de plus près dans notre chapitre sur la rétropropagation.\n",
    "\n",
    "Nous sommes maintenant prêts à écrire le code pour adapter les poids, ce qui signifie entraîner le réseau. Pour ce faire, nous ajoutons une méthode 'adjust' à notre classe Perceptron. La tâche de cette méthode est de corriger l'erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2cd78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 weights,\n",
    "                 learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        'weights' can be a numpy array, list or a tuple with the\n",
    "        actual values of the weights. The number of input values\n",
    "        is indirectly defined by the length of 'weights'\n",
    "        \"\"\"\n",
    "        self.weights = np.array(weights)\n",
    "        self.learning_rate = learning_rate\n",
    "     \n",
    "    # activation function:\n",
    "    @staticmethod\n",
    "    def unit_step_function(x):\n",
    "        if  x < 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    def __call__(self, in_data):\n",
    "        weighted_input = self.weights * in_data\n",
    "        weighted_sum = weighted_input.sum()\n",
    "        return Perceptron.unit_step_function(weighted_sum)\n",
    "    \n",
    "    def adjust(self, \n",
    "               target_result, \n",
    "               calculated_result,\n",
    "               in_data):\n",
    "        if type(in_data) != np.ndarray:\n",
    "            in_data = np.array(in_data)  \n",
    "        error = target_result - calculated_result\n",
    "        if error != 0:\n",
    "            correction = error * in_data * self.learning_rate\n",
    "            self.weights += correction \n",
    "            \n",
    "    def evaluate(self, data, labels):\n",
    "        evaluation = Counter()\n",
    "        for index in range(len(data)):\n",
    "            label = int(round(p(data[index]),0))\n",
    "            if label == labels[index]:\n",
    "                evaluation[\"correct\"] += 1\n",
    "            else:\n",
    "                evaluation[\"wrong\"] += 1\n",
    "        return evaluation\n",
    "                \n",
    "\n",
    "p = Perceptron(weights=[0.1, 0.1],\n",
    "               learning_rate=0.3)\n",
    "\n",
    "for index in range(len(train_data)):\n",
    "    p.adjust(train_labels[index], \n",
    "             p(train_data[index]), \n",
    "             train_data[index])\n",
    "    \n",
    "evaluation = p.evaluate(train_data, train_labels)\n",
    "print(evaluation.most_common())\n",
    "evaluation = p.evaluate(test_data, test_labels)\n",
    "print(evaluation.most_common())\n",
    "\n",
    "print(p.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5440df6e",
   "metadata": {},
   "source": [
    "Tant sur les données d'apprentissage que sur les données de test, nous n'avons que des valeurs correctes, c'est-à-dire que notre réseau a été capable d'apprendre automatiquement et avec succès !\n",
    "\n",
    "Nous visualisons la frontière de décision avec le programme suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "X = np.arange(0, 7)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "lemons = [train_data[i] for i in range(len(train_data)) if train_labels[i] == 1]\n",
    "lemons_x, lemons_y = zip(*lemons)\n",
    "oranges = [train_data[i] for i in range(len(train_data)) if train_labels[i] == 0]\n",
    "oranges_x, oranges_y = zip(*oranges)\n",
    "\n",
    "ax.scatter(oranges_x, oranges_y, c=\"orange\")\n",
    "ax.scatter(lemons_x, lemons_y, c=\"y\")\n",
    "\n",
    "w1 = p.weights[0]\n",
    "w2 = p.weights[1]\n",
    "m = -w1 / w2\n",
    "ax.plot(X, m * X, label=\"decision boundary\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "print(p.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aca406",
   "metadata": {},
   "source": [
    "Examinons l'algorithme \"en action\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f0679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "p = Perceptron(weights=[0.1, 0.1],\n",
    "               learning_rate=0.1)\n",
    "number_of_colors = 10\n",
    "colors = cm.rainbow(np.linspace(0, 1, number_of_colors))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xticks(range(8))\n",
    "ax.set_ylim([-2, 8])\n",
    "\n",
    "counter = 0\n",
    "for index in range(len(train_data)):\n",
    "    old_weights = p.weights.copy()\n",
    "    p.adjust(train_labels[index], \n",
    "             p(train_data[index]), \n",
    "             train_data[index])\n",
    "    if not np.array_equal(old_weights, p.weights):\n",
    "        color = \"orange\" if train_labels[index] == 0 else \"y\"        \n",
    "        ax.scatter(train_data[index][0], \n",
    "                   train_data[index][1],\n",
    "                   color=color)\n",
    "        ax.annotate(str(counter), \n",
    "                    (train_data[index][0], train_data[index][1]))\n",
    "        m = -p.weights[0] / p.weights[1]\n",
    "        print(index, m, p.weights, train_data[index])\n",
    "        ax.plot(X, m * X, label=str(counter), color=colors[counter])\n",
    "        counter += 1\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d23f1e",
   "metadata": {},
   "source": [
    "Chacun des points du diagramme ci-dessus entraîne une modification des poids. Nous les voyons numérotés dans l'ordre de leur apparition et la ligne droite correspondante. De cette façon, nous pouvons voir comment les réseaux \"apprennent\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c21473",
   "metadata": {},
   "source": [
    "[Suivant](12_reseau_de_neurones_simple_python.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
