{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d49b4160",
   "metadata": {},
   "source": [
    "# 12 - Un réseau de neurones simple à partir de zéro en Python\n",
    "\n",
    "\n",
    "## Ensembles de données linéairement séparables\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/illustration12_1.png\" width=\"35%\">\n",
    "</center>\n",
    "\n",
    "Comme nous l'avons montré dans le chapitre précédent de notre tutoriel sur l'apprentissage automatique, un réseau neuronal composé d'un seul perceptron a suffi à séparer les classes de notre exemple. Bien entendu, nous avons soigneusement conçu ces classes pour que cela fonctionne. Il existe de nombreux groupes de classes pour lesquels cela ne fonctionnera pas. Nous allons examiner d'autres exemples et discuter des cas où il ne sera pas possible de séparer les classes.\n",
    "\n",
    "Nos classes ont été linéairement séparables. La séparabilité linéaire a un sens en géométrie euclidienne. Deux ensembles de points (ou classes) sont appelés linéairement séparables, si au moins une ligne droite dans le plan existe de sorte que tous les points d'une classe sont d'un côté de la ligne et tous les points de l'autre classe sont de l'autre côté.\n",
    "\n",
    "Plus formellement :\n",
    "\n",
    "Si deux clusters (classes) de données peuvent être séparés par une frontière de décision sous la forme d'une équation linéaire:\n",
    "\n",
    "$$\\sum_{i=1}^n x_i\\cdot \\omega_1 = 0$$\n",
    "\n",
    "elles sont dites linéairement séparables.\n",
    "\n",
    "Dans le cas contraire, c'est-à-dire si une telle frontière de décision n'existe pas, les deux classes sont dites linéairement inséparables. Dans ce cas, on ne peut pas utiliser un simple réseau de neurones.\n",
    "\n",
    "## Perceptron pour la fonction AND\n",
    "\n",
    "Dans notre prochain exemple, nous allons programmer un réseau de neurones en Python qui implémente la fonction logique \"Et\". Elle est définie pour deux entrées de la manière suivante :\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/illustration12_2.png\" width=\"25%\">\n",
    "</center>\n",
    "\n",
    "Nous avons appris dans le chapitre précédent qu'un réseau neuronal avec un perceptron et deux valeurs d'entrée peut être interprété comme une frontière de décision, c'est-à-dire une ligne droite divisant deux classes. Les deux classes que nous voulons classer dans notre exemple ressemblent à ceci :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48261c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xmin, xmax = -0.2, 1.4\n",
    "X = np.arange(xmin, xmax, 0.1)\n",
    "ax.scatter(0, 0, color=\"r\")\n",
    "ax.scatter(0, 1, color=\"r\")\n",
    "ax.scatter(1, 0, color=\"r\")\n",
    "ax.scatter(1, 1, color=\"g\")\n",
    "ax.set_xlim([xmin, xmax])\n",
    "ax.set_ylim([-0.1, 1.1])\n",
    "m = -1\n",
    "#ax.plot(X, m * X + 1.2, label=\"decision boundary\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ca88be",
   "metadata": {},
   "source": [
    "Nous avons également découvert qu'un réseau neuronal aussi primitif n'est capable de créer que des lignes droites passant par l'origine. Donc des lignes de division comme celle-ci :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e4d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xmin, xmax = -0.2, 1.4\n",
    "X = np.arange(xmin, xmax, 0.1)\n",
    "ax.set_xlim([xmin, xmax])\n",
    "ax.set_ylim([-0.1, 1.1])\n",
    "m = -1\n",
    "for m in np.arange(0, 6, 0.1):\n",
    "    ax.plot(X, m * X )\n",
    "ax.scatter(0, 0, color=\"r\")\n",
    "ax.scatter(0, 1, color=\"r\")\n",
    "ax.scatter(1, 0, color=\"r\")\n",
    "ax.scatter(1, 1, color=\"g\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99ebdc",
   "metadata": {},
   "source": [
    "Nous pouvons voir qu'aucune de ces lignes droites ne peut être utilisée comme limite de décision, ni aucune autre ligne passant par l'origine.\n",
    "\n",
    "Nous avons besoin d'une ligne\n",
    "$$y=m\\cdot x + c$$\n",
    "\n",
    "où l'ordonnée à l'origine $c$ n'est pas égale à 0.\n",
    "\n",
    "Par exemple, la droite\n",
    "$$y=-x + 1,2$$\n",
    "\n",
    "pourrait être utilisé comme ligne de séparation pour notre problème :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c7270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xmin, xmax = -0.2, 1.4\n",
    "X = np.arange(xmin, xmax, 0.1)\n",
    "ax.scatter(0, 0, color=\"r\")\n",
    "ax.scatter(0, 1, color=\"r\")\n",
    "ax.scatter(1, 0, color=\"r\")\n",
    "ax.scatter(1, 1, color=\"g\")\n",
    "ax.set_xlim([xmin, xmax])\n",
    "ax.set_ylim([-0.1, 1.1])\n",
    "m, c = -1, 1.2\n",
    "ax.plot(X, m * X + c )\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665b6ba3",
   "metadata": {},
   "source": [
    "La question est maintenant de savoir si nous pouvons trouver une solution avec des modifications mineures de notre modèle de réseau ? Ou en d'autres termes : Pouvons-nous créer un perceptron capable de définir des frontières de décision arbitraires ?\n",
    "\n",
    "La solution consiste en l'ajout d'un nœud de biais.\n",
    "\n",
    "## Perceptron simple avec un biais\n",
    "Un perceptron avec deux valeurs d'entrée et un biais correspond à une ligne droite générale. À l'aide de la valeur de biais b, nous pouvons entraîner le perceptron à déterminer une frontière de décision avec une interception non nulle c.\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/illustration12_3.png\" width=\"40%\">\n",
    "</center>\n",
    "\n",
    "Alors que les valeurs d'entrée peuvent changer, une valeur de biais reste toujours constante. Seul le poids du nœud de biais peut être adapté.\n",
    "\n",
    "Maintenant, l'équation linéaire d'un perceptron contient un biais :\n",
    "\n",
    "$$\\sum_{i=1}^n \\omega_i\\ x_i +\\omega_{n+1}\\cdot b =0$$\n",
    "\n",
    "Dans notre cas, cela ressemble à ceci :\n",
    "\n",
    "$$\\omega_1\\cdot x_1 + \\omega_2\\cdot x_2 + \\omega_3\\cdot b=0$$\n",
    "\n",
    "ceci est équivalent à\n",
    "$$x_2 = -\\frac{\\omega_1}{\\omega_2}\\cdot x_1-\\frac{\\omega_3}{\\omega_2}\\cdot b$$\n",
    "\n",
    "Cela signifie :\n",
    "\n",
    "$$m=-\\frac{\\omega_1}{\\omega_2}$$\n",
    "\n",
    "et \n",
    "\n",
    "$$c=-\\frac{\\omega_3}{\\omega_2}\\cdot b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c41cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%writefile perceptrons.py\n",
    " \n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 weights,\n",
    "                 bias=1,\n",
    "                 learning_rate=0.3):\n",
    "        \"\"\"\n",
    "        'weights' can be a numpy array, list or a tuple with the\n",
    "        actual values of the weights. The number of input values\n",
    "        is indirectly defined by the length of 'weights'\n",
    "        \"\"\"\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = bias\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    @staticmethod\n",
    "    def unit_step_function(x):\n",
    "        if  x <= 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    def __call__(self, in_data):\n",
    "        in_data = np.concatenate( (in_data, [self.bias]) )\n",
    "        result = self.weights @ in_data\n",
    "        return Perceptron.unit_step_function(result)\n",
    "    \n",
    "    def adjust(self, \n",
    "               target_result, \n",
    "               in_data):\n",
    "        if type(in_data) != np.ndarray:\n",
    "            in_data = np.array(in_data)  # \n",
    "        calculated_result = self(in_data)\n",
    "        error = target_result - calculated_result\n",
    "        if error != 0:\n",
    "            in_data = np.concatenate( (in_data, [self.bias]) )\n",
    "            correction = error * in_data * self.learning_rate\n",
    "            self.weights += correction\n",
    "            \n",
    "    def evaluate(self, data, labels):\n",
    "        evaluation = Counter()\n",
    "        for sample, label in zip(data, labels):\n",
    "            result = self(sample) # predict\n",
    "            if result == label:\n",
    "                evaluation[\"correct\"] += 1\n",
    "            else:\n",
    "                evaluation[\"wrong\"] += 1\n",
    "        return evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aef337",
   "metadata": {},
   "source": [
    "Nous supposons que le code Python ci-dessus avec la classe Perceptron est stocké dans votre répertoire de travail actuel sous le nom ```perceptrons.py```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69a424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from perceptrons import Perceptron\n",
    "\n",
    "def labelled_samples(n):\n",
    "    for _ in range(n):\n",
    "        s = np.random.randint(0, 2, (2,))\n",
    "        yield (s, 1) if s[0] == 1 and s[1] == 1 else (s, 0)\n",
    "\n",
    "p = Perceptron(weights=[0.3, 0.3, 0.3],\n",
    "               learning_rate=0.2)\n",
    "\n",
    "for in_data, label in labelled_samples(30):\n",
    "    p.adjust(label, \n",
    "             in_data)\n",
    "\n",
    "test_data, test_labels = list(zip(*labelled_samples(30)))\n",
    "\n",
    "evaluation = p.evaluate(test_data, test_labels)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e471a839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xmin, xmax = -0.2, 1.4\n",
    "X = np.arange(xmin, xmax, 0.1)\n",
    "ax.scatter(0, 0, color=\"r\")\n",
    "ax.scatter(0, 1, color=\"r\")\n",
    "ax.scatter(1, 0, color=\"r\")\n",
    "ax.scatter(1, 1, color=\"g\")\n",
    "ax.set_xlim([xmin, xmax])\n",
    "ax.set_ylim([-0.1, 1.1])\n",
    "m = -p.weights[0] / p.weights[1]\n",
    "c = -p.weights[2] / p.weights[1]\n",
    "print(m, c)\n",
    "ax.plot(X, m * X + c )\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f8d7bd",
   "metadata": {},
   "source": [
    "Nous allons créer un autre exemple avec des ensembles de données linéairement séparables, qui nécessitent un nœud de biais pour être séparables. Nous allons utiliser la fonction ```make_blobs``` de ```sklearn.datasets``` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a87f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "n_samples = 1000\n",
    "samples, labels = make_blobs(n_samples=n_samples, \n",
    "                             centers=([2.5, 3], [6.7, 7.9]), \n",
    "                             cluster_std=1.4,\n",
    "                             random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f593e9dd",
   "metadata": {},
   "source": [
    "Visualisons les données précédemment créées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea8d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colours = ('green', 'magenta', 'blue', 'cyan', 'yellow', 'red')\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "for n_class in range(2):\n",
    "    ax.scatter(samples[labels==n_class][:, 0], samples[labels==n_class][:, 1], \n",
    "               c=colours[n_class], s=40, label=str(n_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2fdb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "res = train_test_split(samples, labels, \n",
    "                       train_size=0.8,\n",
    "                       test_size=0.2,\n",
    "                       random_state=1)\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = res \n",
    "from perceptrons import Perceptron\n",
    "\n",
    "p = Perceptron(weights=[0.3, 0.3, 0.3],\n",
    "               learning_rate=0.8)\n",
    "\n",
    "for sample, label in zip(train_data, train_labels):\n",
    "    p.adjust(label,\n",
    "             sample)\n",
    "\n",
    "evaluation = p.evaluate(train_data, train_labels)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef311a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = p.evaluate(test_data, test_labels)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466e289d",
   "metadata": {},
   "source": [
    "Visualisons la frontière de décision :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6507f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# plotting learn data\n",
    "colours = ('green', 'blue')\n",
    "for n_class in range(2):\n",
    "    ax.scatter(train_data[train_labels==n_class][:, 0], \n",
    "               train_data[train_labels==n_class][:, 1], \n",
    "               c=colours[n_class], s=40, label=str(n_class))\n",
    "    \n",
    "# plotting test data\n",
    "colours = ('lightgreen', 'lightblue')\n",
    "for n_class in range(2):\n",
    "    ax.scatter(test_data[test_labels==n_class][:, 0], \n",
    "               test_data[test_labels==n_class][:, 1], \n",
    "               c=colours[n_class], s=40, label=str(n_class))\n",
    "\n",
    "\n",
    "    \n",
    "X = np.arange(np.max(samples[:,0]))\n",
    "m = -p.weights[0] / p.weights[1]\n",
    "c = -p.weights[2] / p.weights[1]\n",
    "print(m, c)\n",
    "ax.plot(X, m * X + c )\n",
    "plt.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46f556c",
   "metadata": {},
   "source": [
    "Dans la section suivante, nous allons présenter le problème XOR pour les réseaux neuronaux. Il s'agit de l'exemple le plus simple de réseau neuronal non linéairement séparable. Il peut être résolu à l'aide d'une couche supplémentaire de neurones, appelée couche cachée.\n",
    "\n",
    "## Le problème XOR pour les réseaux neuronaux\n",
    "\n",
    "La fonction XOR (ou exclusif) est définie par la table de vérité suivante :\n",
    "<center>\n",
    "    <img src=\"img/illustration12_4.png\" width=\"30%\">\n",
    "</center>\n",
    "\n",
    "Ce problème ne peut pas être résolu avec un simple réseau de neurones, comme on peut le voir dans le diagramme suivant :\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/illustration12_5.png\" width=\"30%\">\n",
    "</center>\n",
    "\n",
    "Quelle que soit la ligne droite que vous choisissez, vous ne réussirez jamais à avoir les points bleus d'un côté et les points orange de l'autre. C'est ce que montre la figure suivante. Les points orange sont sur la ligne orange. Cela signifie qu'il ne peut s'agir d'une ligne de séparation. Si nous déplaçons cette ligne parallèlement - quelle que soit la direction - il y aura toujours deux points orange et un point bleu d'un côté et un seul point bleu de l'autre côté. Si nous déplaçons la ligne orange de manière non parallèle, il y aura un point bleu et un point orange de chaque côté, sauf si la ligne passe par un point orange. Il est donc impossible qu'une seule ligne droite sépare ces points.\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/illustration12_6.png\" width=\"30%\">\n",
    "</center>\n",
    "\n",
    "Pour résoudre ce problème, nous devons introduire un nouveau type de réseaux neuronaux, un réseau avec des couches dites cachées. Une couche cachée permet au réseau de réorganiser ou de réarranger les données d'entrée.\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/illustration12_7.png\" width=\"50%\">\n",
    "</center>\n",
    "\n",
    "Nous n'aurons besoin que d'une couche cachée avec deux neurones. L'un fonctionne comme une porte ET et l'autre comme une porte OU. La sortie se déclenchera lorsque la porte OU se déclenchera et que la porte ET ne se déclenchera pas.\n",
    "\n",
    "Comme nous l'avons déjà mentionné, nous ne pouvons pas trouver une ligne qui sépare les points orange des points bleus. Mais ils peuvent être séparés par deux lignes, par exemple L1 et L2 dans le schéma suivant :\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/illustration12_8.png\" width=\"30%\">\n",
    "</center>\n",
    "\n",
    "Pour résoudre ce problème, nous avons besoin d'un réseau du type suivant, c'est-à-dire avec une couche cachée N1 et N2\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/illustration12_9.png\" width=\"20%\">\n",
    "</center>\n",
    "\n",
    "Le neurone N1 va déterminer une ligne, par exemple L1 et le neurone N2 va déterminer l'autre ligne L2. N3 résoudra finalement notre problème :\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/illustration12_10.png\" width=\"40%\">\n",
    "</center>\n",
    "\n",
    "L'implémentation de cette méthode en Python devra attendre le prochain chapitre de notre tutoriel sur l'apprentissage automatique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d43cdd",
   "metadata": {},
   "source": [
    "## Exercices\n",
    "\n",
    "### Exercice 1\n",
    "\n",
    "Nous pourrions étendre le ET logique aux valeurs flottantes entre 0 et 1 de la manière suivante :\n",
    "<center>\n",
    "    <img src=\"img/illustration12_11.png\" width=\"30%\">\n",
    "</center>\n",
    "\n",
    "Essayez de former un réseau neuronal avec un seul perceptron. Pourquoi cela ne fonctionne-t-il pas ?\n",
    "\n",
    "### Exercice 2\n",
    "Un point appartient à une classe 0, si $x_1<0.5$ et appartient à la classe 1, si $x_1 \\geq 0.5$. Entraînez un réseau avec un perceptron pour classer des points arbitraires. Que pouvez-vous dire de la limite de décision ? Qu'en est-il des valeurs d'entrée $x_2$\n",
    "\n",
    "### Solutions aux exercices\n",
    "\n",
    "__Solution du 1er exercice__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106f1575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from perceptrons import Perceptron\n",
    "\n",
    "p = Perceptron(weights=[0.3, 0.3, 0.3],\n",
    "               bias=1,\n",
    "               learning_rate=0.2)\n",
    "\n",
    "def labelled_samples(n):\n",
    "    for _ in range(n):\n",
    "        s = np.random.random((2,))\n",
    "        yield (s, 1) if s[0] >= 0.5 and s[1] >= 0.5 else (s, 0)\n",
    "\n",
    "for in_data, label in labelled_samples(30):\n",
    "    p.adjust(label, \n",
    "             in_data)\n",
    "\n",
    "test_data, test_labels = list(zip(*labelled_samples(60)))\n",
    "\n",
    "evaluation = p.evaluate(test_data, test_labels)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403d8573",
   "metadata": {},
   "source": [
    "Le moyen le plus simple de voir pourquoi cela ne fonctionne pas est de visualiser les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea1974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ones = [test_data[i] for i in range(len(test_data)) if test_labels[i] == 1]\n",
    "zeroes = [test_data[i] for i in range(len(test_data)) if test_labels[i] == 0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xmin, xmax = -0.2, 1.2\n",
    "X, Y = list(zip(*ones))\n",
    "ax.scatter(X, Y, color=\"g\")\n",
    "X, Y = list(zip(*zeroes))\n",
    "ax.scatter(X, Y, color=\"r\")\n",
    "ax.set_xlim([xmin, xmax])\n",
    "ax.set_ylim([-0.1, 1.1])\n",
    "c = -p.weights[2] / p.weights[1]\n",
    "m = -p.weights[0] / p.weights[1]\n",
    "X = np.arange(xmin, xmax, 0.1)\n",
    "ax.plot(X, m * X + c, label=\"decision boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfed607f",
   "metadata": {},
   "source": [
    "On voit que les points verts et les points rouges ne sont pas séparables par une seule droite.\n",
    "\n",
    "___Solution du 2ème exercice___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93d4896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from perceptrons import Perceptron\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def labelled_samples(n):\n",
    "    for _ in range(n):\n",
    "        s = np.random.random((2,))\n",
    "        yield (s, 0) if s[0] < 0.5 else (s, 1)\n",
    "\n",
    "\n",
    "p = Perceptron(weights=[0.3, 0.3, 0.3],\n",
    "               learning_rate=0.4)\n",
    "\n",
    "for in_data, label in labelled_samples(300):\n",
    "    p.adjust(label, \n",
    "             in_data)\n",
    "\n",
    "test_data, test_labels = list(zip(*labelled_samples(500)))\n",
    "\n",
    "print(p.weights)\n",
    "p.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ones = [test_data[i] for i in range(len(test_data)) if test_labels[i] == 1]\n",
    "zeroes = [test_data[i] for i in range(len(test_data)) if test_labels[i] == 0]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xmin, xmax = -0.2, 1.2\n",
    "X, Y = list(zip(*ones))\n",
    "ax.scatter(X, Y, color=\"g\")\n",
    "X, Y = list(zip(*zeroes))\n",
    "ax.scatter(X, Y, color=\"r\")\n",
    "ax.set_xlim([xmin, xmax])\n",
    "ax.set_ylim([-0.1, 1.1])\n",
    "c = -p.weights[2] / p.weights[1]\n",
    "m = -p.weights[0] / p.weights[1]\n",
    "X = np.arange(xmin, xmax, 0.1)\n",
    "ax.plot(X, m * X + c, label=\"decision boundary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fb8d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.weights, m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54701d89",
   "metadata": {},
   "source": [
    "La pente m devra être de plus en plus grande dans des situations comme celle-ci."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a12b07",
   "metadata": {},
   "source": [
    "[Suivant](13_classe_perceptron_sklearn.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
