{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b39c9de0",
   "metadata": {},
   "source": [
    "# 14 - Réseaux neuronaux, structure, poids et matrices\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Nous avons présenté les idées de base sur les réseaux neuronaux dans le chapitre précédent de notre tutoriel sur l'apprentissage automatique.\n",
    "\n",
    "Nous avons souligné la similitude entre les neurones et les réseaux de neurones en biologie. Nous avons également présenté de très petits réseaux neuronaux artisanaux et introduit les frontières de décision et le problème XOR.\n",
    "\n",
    "Dans les exemples simples que nous avons présentés jusqu'à présent, nous avons vu que les poids sont les éléments essentiels d'un réseau neuronal. Avant de commencer à écrire un réseau neuronal à couches multiples, nous devons examiner de plus près les poids.\n",
    "\n",
    "Nous devons voir comment initialiser les poids et comment multiplier efficacement les poids avec les valeurs d'entrée.\n",
    "\n",
    "Dans les chapitres suivants, nous allons concevoir un réseau neuronal en Python, qui se compose de trois couches, à savoir la couche d'entrée, une couche cachée et une couche de sortie. Vous pouvez voir la structure de ce réseau neuronal dans le diagramme suivant. Nous avons une couche d'entrée avec trois nœuds $i_1$, $i_2$, $i_3$. Ces noeuds reçoivent les valeurs d'entrée correspondantes $x_1$, $x_2$, $x_3$. La couche intermédiaire ou cachée comporte quatre nœuds $h_1$, $h_2$, $h_3$, $h_4$. L'entrée de cette couche provient de la couche d'entrée. Nous discuterons bientôt de ce mécanisme. Enfin, notre couche de sortie se compose des deux noeuds suivants $o_1$ et $o_2$.\n",
    "\n",
    "La couche d'entrée est différente des autres couches. Les nœuds de la couche d'entrée sont passifs. Cela signifie que les neurones d'entrée ne modifient pas les données, c'est-à-dire qu'aucun poids n'est utilisé dans ce cas. Ils reçoivent une valeur unique et dupliquent cette valeur à leurs nombreuses sorties.\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/illustration14_1.png\" width=\"50%\">\n",
    "</center>    \n",
    "\n",
    "La couche d'entrée est constituée des nœuds $i_1$, $i_2$ et $i_3$. En principe, l'entrée est un vecteur unidimensionnel, comme (2, 4, 11). Un vecteur unidimensionnel est représenté dans ```numpy``` comme ceci :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483b2974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_vector = np.array([2, 4, 11])\n",
    "print(input_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a67a64a",
   "metadata": {},
   "source": [
    "Dans l'algorithme, que nous écrirons plus tard, nous devrons le transposer en un vecteur colonne, c'est-à-dire un tableau bidimensionnel avec une seule colonne :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319b0631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_vector = np.array([2, 4, 11])\n",
    "input_vector = np.array(input_vector, ndmin=2).T\n",
    "print(\"The input vector:\\n\", input_vector)\n",
    "\n",
    "print(\"The shape of this vector: \", input_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dfec36",
   "metadata": {},
   "source": [
    "## Poids et matrices\n",
    "\n",
    "À chacune des flèches de notre diagramme de réseau est associée une valeur de poids. Pour l'instant, nous n'examinerons que les flèches entre la couche d'entrée et la couche de sortie.\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/illustration14_2.png\" width=\"50%\">\n",
    "</center> \n",
    "\n",
    "La valeur $x_1$ entrant dans le nœud $i_1$ sera distribué en fonction des valeurs des poids. Dans le diagramme suivant, nous avons ajouté quelques valeurs d'exemple. En utilisant ces valeurs, les valeurs d'entrée $(Ih_1,Ih_2,Ih_3,Ih_4)$ dans les noeuds $(h_1,h_2,h_3,h_4)$ de la couche cachée peut être calculée comme suit :\n",
    "<center>\n",
    "    <img src=\"img/illustration14_3.png\" width=\"50%\">\n",
    "</center> \n",
    "\n",
    "Ceux qui sont familiers avec les matrices et la multiplication matricielle verront à quoi cela se résume. Nous allons redessiner notre réseau et désigner les poids par $w_{ij}$:\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/illustration14_4.png\" width=\"50%\">\n",
    "</center> \n",
    "\n",
    "Afin d'exécuter efficacement tous les calculs nécessaires, nous allons organiser les poids dans une matrice de poids. Dans le diagramme ci-dessus, les poids forment un tableau, que nous appellerons \"poids_in_hidden\" dans notre classe de réseau neuronal. Ce nom doit indiquer que les poids relient les nœuds d'entrée et les nœuds cachés, c'est-à-dire qu'ils se trouvent entre l'entrée et la couche cachée. Nous abrégerons également le nom par \" wih \". La matrice des poids entre la couche cachée et la couche de sortie sera désignée par \"who\" :\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/illustration14_5.png\" width=\"40%\">\n",
    "</center> \n",
    "\n",
    "Maintenant que nous avons défini nos matrices de poids, nous devons passer à l'étape suivante. Nous devons multiplier la matrice par le vecteur d'entrée. En fait, c'est exactement ce que nous avons fait manuellement dans notre exemple précédent.\n",
    "\n",
    "$$\\left(\\begin{array}{cc} y_1\\\\y_2\\\\y_3\\\\y_4\\end{array}\\right)=\\left(\\begin{array}{cc} w_{11} & w_{12} & w_{13}\\\\w_{21} & w_{22} & w_{23}\\\\w_{31} & w_{32} & w_{33}\\\\w_{41} &w_{42}& w_{43}\\end{array}\\right)\\left(\\begin{array}{cc} x_1\\\\x_2\\\\x_3\\end{array}\\right)=\\left(\\begin{array}{cc} w_{11} \\cdot x_1 + w_{12} \\cdot x_2 + w_{13} \\cdot x_3\\\\w_{21} \\cdot x_1 + w_{22} \\cdot x_2 + w_{23} \\cdot x_3\\\\w_{31} \\cdot x_1 + w_{32} \\cdot x_2 + w_{33}\\cdot x_3\\\\w_{41} \\cdot x_1 + w_{42} \\cdot x_2 + w_{43} \\cdot x_3\\end{array}\\right)$$\n",
    "\n",
    "Nous avons une situation similaire pour la matrice 'qui' entre la couche cachée et la couche de sortie. Ainsi, la sortie $z_1$ et $z_2$ à partir des noeuds $o_1$ et $o_2$ peuvent également être calculés à l'aide de multiplications matricielles :\n",
    "\n",
    "$$\\left(\\begin{array}{cc} z_1\\\\z_2\\end{array}\\right)=\\left(\\begin{array}{cc} wh_{11} & wh_{12} & wh_{13} & wh_{14}\\\\wh_{21} & wh_{22} & wh_{23} & wh_{24}\\end{array}\\right)\\left(\\begin{array}{cc} y_1\\\\y_2\\\\y_3\\\\y_4\\end{array}\\right)=\\left(\\begin{array}{cc} wh_{11} \\cdot y_1 + wh_{12} \\cdot y_2 + wh_{13} \\cdot y_3 + wh_{14} \\cdot y_4\\\\wh_{21} \\cdot y_1 + wh_{22} \\cdot y_2 + wh_{23} \\cdot y_3 + wh_{24} \\cdot y_4\\end{array}\\right)$$\n",
    "\n",
    "Vous avez peut-être remarqué qu'il manque quelque chose dans nos calculs précédents. Nous avons montré dans notre chapitre d'introduction Réseaux de neurones à partir de zéro en Python que nous devons appliquer une fonction d'activation ou de pas $\\Phi$ sur chacune de ces sommes.\n",
    "\n",
    "L'image suivante représente l'ensemble du flux de calcul, c'est-à-dire la multiplication matricielle et l'application successive de la fonction d'activation.\n",
    "\n",
    "La multiplication matricielle entre la matrice wih et la matrice des valeurs des nœuds d'entrée $x_1,x_2,x_3$ calcule la sortie qui sera transmise à la fonction d'activation.\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/illustration14_7.png\" width=\"60%\">\n",
    "</center> \n",
    "\n",
    "La sortie finale $y_1,y_2,y_3,y_4$ est l'entrée de la matrice de poids qui :\n",
    "\n",
    "Même si le traitement est complètement analogique, nous allons également examiner en détail ce qui se passe entre notre couche cachée et la couche de sortie :\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/illustration14_8.png\" width=\"60%\">\n",
    "</center> \n",
    "\n",
    "## Initialisation des matrices de poids\n",
    "\n",
    "L'un des choix importants à faire avant de former un réseau neuronal consiste à initialiser les matrices de poids. Nous ne savons rien des poids possibles, lorsque nous commençons. Nous pourrions donc commencer par des valeurs arbitraires ?\n",
    "\n",
    "Comme nous l'avons vu, l'entrée de tous les nœuds, à l'exception des nœuds d'entrée, est calculée en appliquant la fonction d'activation à la somme suivante :\n",
    "\n",
    "$$y_j = \\sum_{i=1}^{n} w_{ji} \\cdot x_i$$\n",
    "\n",
    "(avec n étant le nombre de nœuds dans la couche précédente et $y_j$ est l'entrée d'un nœud de la couche suivante)\n",
    "\n",
    "Nous pouvons facilement voir que ce ne serait pas une bonne idée de mettre toutes les valeurs de poids à 0, car dans ce cas, le résultat de cette sommation sera toujours zéro. Cela signifie que notre réseau sera incapable d'apprendre. Il s'agit du pire choix, mais initialiser une matrice de poids à 1 est également un mauvais choix.\n",
    "\n",
    "Les valeurs des matrices de poids doivent être choisies de manière aléatoire et non arbitraire. En choisissant une distribution normale aléatoire, nous avons brisé des situations symétriques possibles, qui peuvent et sont souvent mauvaises pour le processus d'apprentissage.\n",
    "\n",
    "Il existe plusieurs façons d'initialiser les matrices de poids de manière aléatoire. La première que nous allons présenter est la fonction unity de numpy.random. Elle crée des échantillons qui sont uniformément distribués sur l'intervalle semi-ouvert ```[low, high]```, ce qui signifie que la valeur basse est incluse et la valeur haute est exclue. Chaque valeur dans l'intervalle donné a la même probabilité d'être tirée par 'uniform'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ec34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "number_of_samples = 1200\n",
    "low = -1\n",
    "high = 0\n",
    "s = np.random.uniform(low, high, number_of_samples)\n",
    "\n",
    "# all values of s are within the half open interval [-1, 0) :\n",
    "print(np.all(s >= -1) and np.all(s < 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094c01d0",
   "metadata": {},
   "source": [
    "L'histogramme des échantillons, créé avec la fonction uniforme dans notre exemple précédent, ressemble à ceci :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877005a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed46b7d",
   "metadata": {},
   "source": [
    "La prochaine fonction que nous allons examiner est 'binomial' de numpy.binomial :\n",
    "```python\n",
    "binomial(n, p, size=None)\n",
    "```\n",
    "\n",
    "Elle tire des échantillons d'une distribution binomiale avec les paramètres spécifiés, n essais et la probabilité p de succès où n est un entier >= 0 et p est un flottant dans l'intervalle [0,1]. (n peut être entré sous la forme d'un flottant, mais il est tronqué en entier lors de son utilisation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac850b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.random.binomial(100, 0.5, 1200)\n",
    "plt.hist(s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13049800",
   "metadata": {},
   "source": [
    "Nous aimons créer des nombres aléatoires avec une distribution normale, mais les nombres doivent être bornés. Ce n'est pas le cas avec np.random.normal(), car il n'offre aucun paramètre de limite.\n",
    "\n",
    "Nous pouvons utiliser ```truncnorm``` de ```scipy.stats``` dans ce but.\n",
    "\n",
    "La forme standard de cette distribution est une normale standard tronquée à l'intervalle ```[a, b]``` - remarquez que a et b sont définis sur le domaine de la normale standard. Pour convertir les valeurs de clip pour une moyenne et un écart-type spécifiques, utilisez :\n",
    "\n",
    "```python\n",
    "a, b = (myclip_a - my_mean) / my_std, (myclip_b - my_mean) / my_std\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import truncnorm\n",
    "\n",
    "s = truncnorm(a=-2/3., b=2/3., scale=1, loc=0).rvs(size=1000)\n",
    "\n",
    "plt.hist(s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4197f",
   "metadata": {},
   "source": [
    "La fonction 'truncnorm' est difficile à utiliser. Pour nous faciliter la vie, nous définissons une fonction truncated_normal dans la suite pour faciliter cette tâche :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f647eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm(\n",
    "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "\n",
    "X = truncated_normal(mean=0, sd=0.4, low=-0.5, upp=0.5)\n",
    "s = X.rvs(10000)\n",
    "\n",
    "plt.hist(s)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e73bce5",
   "metadata": {},
   "source": [
    "Autres exemples :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e21780",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = truncated_normal(mean=2, sd=1, low=1, upp=10)\n",
    "X2 = truncated_normal(mean=5.5, sd=1, low=1, upp=10)\n",
    "X3 = truncated_normal(mean=8, sd=1, low=1, upp=10)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(3, sharex=True)\n",
    "ax[0].hist(X1.rvs(10000), density=True)\n",
    "ax[1].hist(X2.rvs(10000), density=True)\n",
    "ax[2].hist(X3.rvs(10000), density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0041341a",
   "metadata": {},
   "source": [
    "Nous allons maintenant créer la matrice des poids des liens. ```truncated_normal``` est idéal à cet effet. C'est une bonne idée de choisir des valeurs aléatoires à l'intérieur de l'intervalle.\n",
    "\n",
    "$$(-\\frac{1}{\\sqrt{n}}, \\frac{1}{\\sqrt{n}})$$\n",
    "\n",
    "où n désigne le nombre de nœuds d'entrée.\n",
    "\n",
    "Nous pouvons donc créer notre matrice \"wih\" avec :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb85f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_input_nodes = 3\n",
    "no_of_hidden_nodes = 4\n",
    "rad = 1 / np.sqrt(no_of_input_nodes)\n",
    "\n",
    "X = truncated_normal(mean=2, sd=1, low=-rad, upp=rad)\n",
    "wih = X.rvs((no_of_hidden_nodes, no_of_input_nodes))\n",
    "wih"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8087261",
   "metadata": {},
   "source": [
    "De même, nous pouvons maintenant définir la matrice de poids \"qui\" :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad208c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_hidden_nodes = 4\n",
    "no_of_output_nodes = 2\n",
    "rad = 1 / np.sqrt(no_of_hidden_nodes)  # this is the input in this layer!\n",
    "\n",
    "X = truncated_normal(mean=2, sd=1, low=-rad, upp=rad)\n",
    "who = X.rvs((no_of_output_nodes, no_of_hidden_nodes))\n",
    "who"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a444678c",
   "metadata": {},
   "source": [
    "[Suivant](15_reseau_de_neurone_python.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
