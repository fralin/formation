{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3afd8e8",
   "metadata": {},
   "source": [
    "# 15 - Construction d'un réseau de neurones avec Python\n",
    "\n",
    "## Une classe de réseau de neurones\n",
    "\n",
    "\n",
    "Nous avons appris dans le chapitre précédent de notre tutoriel sur les réseaux de neurones les faits les plus importants concernant les poids. Nous avons vu comment ils sont utilisés et comment nous pouvons les implémenter en Python. Nous avons vu que la multiplication des poids avec les valeurs d'entrée peut être réalisée avec des tableaux de Numpy en appliquant la multiplication matricielle.\n",
    "\n",
    "Cependant, ce que nous n'avions pas fait, c'était de les tester dans un environnement réel de réseau neuronal. Nous devons d'abord créer cet environnement. Nous allons maintenant créer une classe en Python, implémentant un réseau neuronal. Nous allons procéder par petites étapes afin que tout soit facile à comprendre.\n",
    "\n",
    "Les méthodes les plus essentielles dont notre classe a besoin sont\n",
    "\n",
    "- ```__init__``` pour initialiser une classe, c'est-à-dire que nous allons définir le nombre de neurones pour chaque couche et initialiser les matrices de poids.\n",
    "- ```run``` : Une méthode qui est appliquée à un échantillon, que nous voulons classer. Elle applique cet échantillon au réseau neuronal. On pourrait dire que l'on \" exécute \" le réseau pour \" prédire \" le résultat. Dans d'autres implémentations, cette méthode est souvent appelée \" predict \".\n",
    "- ```train``` : Cette méthode reçoit en entrée un échantillon et la valeur cible correspondante. Avec cette entrée, elle peut ajuster les valeurs de poids si nécessaire. Cela signifie que le réseau apprend à partir d'une entrée. Du point de vue de l'utilisateur, nous \"entrainons\" le réseau. Dans ```sklearn``` par exemple, cette méthode est appelée ```fit```.\n",
    "\n",
    "Nous remettrons à plus tard la définition de la méthode ```train``` and ```run```. Les matrices de poids doivent être initialisées à l'intérieur de la méthode ```__init__```. Nous le faisons indirectement. Nous définissons une méthode ```create_weight_matrices``` et l'appelons dans ```__init__```. De cette façon, la méthode init reste claire.\n",
    "\n",
    "Nous allons également reporter l'ajout de nœuds de biais aux couches.\n",
    "\n",
    "Le code Python suivant contient une implémentation d'une classe de réseau neuronal appliquant les connaissances que nous avons développées dans le chapitre précédent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cef0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm(\n",
    "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "\n",
    "class NeuralNetwork:\n",
    "   \n",
    "    def __init__(self, \n",
    "                 no_of_in_nodes, \n",
    "                 no_of_out_nodes,  # corresponds to the number of classes\n",
    "                 no_of_hidden_nodes,\n",
    "                 learning_rate):\n",
    "        self.no_of_in_nodes = no_of_in_nodes\n",
    "        self.no_of_out_nodes = no_of_out_nodes \n",
    "        self.no_of_hidden_nodes = no_of_hidden_nodes\n",
    "        self.learning_rate = learning_rate  \n",
    "        self.create_weight_matrices()\n",
    "        \n",
    "    def create_weight_matrices(self):\n",
    "        rad = 1 / np.sqrt(self.no_of_in_nodes)\n",
    "        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
    "        self.weights_in_hidden = X.rvs((self.no_of_hidden_nodes, \n",
    "                                       self.no_of_in_nodes))\n",
    "        rad = 1 / np.sqrt(self.no_of_hidden_nodes)\n",
    "        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
    "        self.weights_hidden_out = X.rvs((self.no_of_out_nodes, \n",
    "                                        self.no_of_hidden_nodes))\n",
    "           \n",
    "    \n",
    "    def train(self):\n",
    "        pass\n",
    "    \n",
    "    def run(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05474bc7",
   "metadata": {},
   "source": [
    "Nous ne pouvons pas faire grand chose avec ce code, mais nous pouvons au moins l'initialiser. Nous pouvons également jeter un coup d'oeil aux matrices de poids :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deea48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_network = NeuralNetwork(no_of_in_nodes = 3, \n",
    "                               no_of_out_nodes = 2, \n",
    "                               no_of_hidden_nodes = 4,\n",
    "                               learning_rate = 0.1)\n",
    "print(simple_network.weights_in_hidden)\n",
    "print(simple_network.weights_hidden_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb5391",
   "metadata": {},
   "source": [
    "## Fonctions d'activation, Sigmoïde et ReLU\n",
    "\n",
    "Avant de pouvoir programmer la méthode d'exécution, nous devons nous occuper de la fonction d'activation. Nous avions le diagramme suivant dans le chapitre d'introduction aux réseaux de neurones :\n",
    "\n",
    "<center>\n",
    "    <img src=\"img/illustration15_1.png\" width=\"50%\">\n",
    "</center>\n",
    "\n",
    "Les valeurs d'entrée d'un perceptron sont traitées par la fonction de sommation et suivies par une fonction d'activation, transformant la sortie de la fonction de sommation en une sortie souhaitée et plus appropriée. La fonction de sommation signifie que nous aurons une multiplication matricielle des vecteurs de poids et des valeurs d'entrée.\n",
    "\n",
    "Il existe de nombreuses fonctions d'activation différentes utilisées dans les réseaux neuronaux. L'une des vues d'ensemble les plus complètes des fonctions d'activation possibles se trouve sur Wikipedia.\n",
    "\n",
    "La fonction sigmoïde est l'une des fonctions d'activation les plus utilisées. La fonction sigmoïde, que nous utilisons, est également connue sous le nom de fonction logistique.\n",
    "\n",
    "Elle est définie comme suit:\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "Regardons le graphique de la fonction sigmoïde. Nous utilisons ```matplotlib``` pour tracer la fonction sigmoïde :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a726e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def sigma(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "X = np.linspace(-5, 5, 100)\n",
    "\n",
    "\n",
    "plt.plot(X, sigma(X),'b')\n",
    "plt.xlabel('X Axis')\n",
    "plt.ylabel('Y Axis')\n",
    "plt.title('Sigmoid Function')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.text(2.3, 0.84, r'$\\sigma(x)=\\frac{1}{1+e^{-x}}$', fontsize=16)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28d5a91",
   "metadata": {},
   "source": [
    "En observant le graphique, nous pouvons voir que la fonction sigmoïde fait correspondre un nombre donné x à une plage de nombres comprise entre 0 et 1. 0 et 1 ne sont pas inclus ! Plus la valeur de x est grande, plus la valeur de la fonction sigmoïde se rapproche de 1 et plus x est petit, plus la valeur de la fonction sigmoïde se rapproche de 0.\n",
    "\n",
    "Au lieu de définir nous-mêmes la fonction sigmoïde, nous pouvons également utiliser la fonction expit de scipy.special, qui est une implémentation de la fonction sigmoïde. Elle peut être appliquée à différentes classes de données comme int, float, list, numpy,ndarray et ainsi de suite. Le résultat est un ndarray de la même forme que les données d'entrée x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678a3f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "print(expit(3.4))\n",
    "print(expit([3, 4, 1]))\n",
    "print(expit(np.array([0.8, 2.3, 8])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb498a49",
   "metadata": {},
   "source": [
    "La fonction logistique est souvent utilisée dans les réseaux neuronaux pour introduire une non-linéarité dans le modèle et pour adapter les signaux dans une plage spécifiée, c'est-à-dire 0 et 1. Elle est également trés utilisée car  dérivable, ce qui est nécessaire dans la méthode de rétropropagation.\n",
    "\n",
    "$$\\sigma(x)=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "et sa dérivée :\n",
    "\n",
    "$$\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31801c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def sigma(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "X = np.linspace(-5, 5, 100)\n",
    "\n",
    "plt.plot(X, sigma(X))\n",
    "plt.plot(X, sigma(X) * (1 - sigma(X)))\n",
    "\n",
    "plt.xlabel('X Axis')\n",
    "plt.ylabel('Y Axis')\n",
    "plt.title('Sigmoid Function')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.text(2.3, 0.84, r'$\\sigma(x)=\\frac{1}{1+e^{-x}}$', fontsize=16)\n",
    "plt.text(0.3, 0.1, r'$\\sigma\\'(x) = \\sigma(x)(1 - \\sigma(x))$', fontsize=16)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceca9b71",
   "metadata": {},
   "source": [
    "Nous pouvons également définir notre propre fonction sigmoïde avec le décorateur ```vectorize``` de numpy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d18493",
   "metadata": {},
   "outputs": [],
   "source": [
    "@np.vectorize\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.e ** -x)\n",
    "\n",
    "#sigmoid = np.vectorize(sigmoid)\n",
    "sigmoid([3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020a09ff",
   "metadata": {},
   "source": [
    "Une autre fonction d'activation facile à utiliser est la fonction ReLU. ReLU signifie unité linéaire rectifiée. Elle est également connue sous le nom de fonction de rampe. Elle est définie comme la partie positive de son argument, c'est-à-dire $y = \\max{(0, x)}$. C'est \"actuellement, la fonction d'activation la plus réussie et la plus utilisée est l'unité linéaire rectifiée (ReLU). La fonction ReLu est plus efficace du point de vue du calcul que les fonctions de type Sigmoïde, car Relu signifie seulement choisir le maximum entre 0 et l'argument x, alors que les Sigmoïdes doivent effectuer des opérations exponentielles coûteuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a392e2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative activation function\n",
    "def ReLU(x):\n",
    "    return np.maximum(0.0, x)\n",
    "\n",
    "# derivation of relu\n",
    "def ReLU_derivation(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf6526c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.linspace(-5, 6, 100)\n",
    "plt.plot(X, ReLU(X),'b')\n",
    "plt.xlabel('X Axis')\n",
    "plt.ylabel('Y Axis')\n",
    "plt.title('ReLU Function')\n",
    "plt.grid()\n",
    "plt.text(0.8, 0.4, r'$ReLU(x)=max(0, x)$', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f76559f",
   "metadata": {},
   "source": [
    "## Ajout d'une méthode ```run```\n",
    "\n",
    "\n",
    "Nous avons maintenant tout réuni pour implémenter la méthode run (ou predict) de notre classe de réseau neuronal. Nous allons utiliser scipy.special comme fonction d'activation et la renommer activation_function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f457743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as activation_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fda1f4",
   "metadata": {},
   "source": [
    "Tout ce que nous avons à faire dans la méthode d'exécution est le suivant.\n",
    "\n",
    "1. Multiplication matricielle du vecteur d'entrée et de la matrice weights_in_hidden.\n",
    "2. Application de la fonction d'activation au résultat de l'étape 1.\n",
    "3. Multiplication matricielle du vecteur résultat de l'étape 2 et de la matrice des poids_in_hidden.\n",
    "4. Pour obtenir le résultat final : Application de la fonction d'activation au résultat de l'étape 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db8c618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit as activation_function\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm(\n",
    "        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "           \n",
    "    def __init__(self, \n",
    "                 no_of_in_nodes, \n",
    "                 no_of_out_nodes, \n",
    "                 no_of_hidden_nodes,\n",
    "                 learning_rate):\n",
    "        self.no_of_in_nodes = no_of_in_nodes\n",
    "        self.no_of_out_nodes = no_of_out_nodes\n",
    "        self.no_of_hidden_nodes = no_of_hidden_nodes\n",
    "        self.learning_rate = learning_rate \n",
    "        self.create_weight_matrices()\n",
    "        \n",
    "    def create_weight_matrices(self):\n",
    "        \"\"\" A method to initialize the weight matrices of the neural network\"\"\"\n",
    "        rad = 1 / np.sqrt(self.no_of_in_nodes)\n",
    "        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
    "        self.weights_in_hidden = X.rvs((self.no_of_hidden_nodes, \n",
    "                                       self.no_of_in_nodes))\n",
    "        rad = 1 / np.sqrt(self.no_of_hidden_nodes)\n",
    "        X = truncated_normal(mean=0, sd=1, low=-rad, upp=rad)\n",
    "        self.weights_hidden_out = X.rvs((self.no_of_out_nodes, \n",
    "                                        self.no_of_hidden_nodes))\n",
    "    \n",
    "    \n",
    "    def train(self, input_vector, target_vector):\n",
    "        pass\n",
    "            \n",
    "    \n",
    "    def run(self, input_vector):\n",
    "        \"\"\"\n",
    "        running the network with an input vector 'input_vector'. \n",
    "        'input_vector' can be tuple, list or ndarray\n",
    "        \"\"\"\n",
    "        # turning the input vector into a column vector\n",
    "        # turn one-dimensional into 2-dimensional column vector:\n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        input_hidden = activation_function(self.weights_in_hidden @ input_vector)\n",
    "        output_vector = activation_function(self.weights_hidden_out @ input_hidden)\n",
    "        return output_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc18503",
   "metadata": {},
   "source": [
    "Nous pouvons instancier une instance de cette classe, qui sera un réseau neuronal. Dans l'exemple suivant, nous créons un réseau avec deux nœuds d'entrée, quatre nœuds cachés et deux nœuds de sortie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee64db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_network = NeuralNetwork(no_of_in_nodes=2, \n",
    "                               no_of_out_nodes=2, \n",
    "                               no_of_hidden_nodes=4,\n",
    "                               learning_rate=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ecd635",
   "metadata": {},
   "source": [
    "Nous pouvons appliquer la méthode run à tous les tableaux dont la forme est (2,), ainsi qu'aux listes et aux tuples comportant deux éléments numériques. Le résultat de l'appel est défini par les valeurs aléatoires des poids :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04140a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_network.run([(3, 4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d5e6e",
   "metadata": {},
   "source": [
    "[Suivant](16_retropropagation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
