{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4a3b38-b944-44b9-bfad-314b3540de7e",
   "metadata": {},
   "source": [
    "# 8. Classificateur k-Nearest Neighbor en Python\n",
    "\n",
    "<center><img src=\"img/illustration8_1.png\" width=\"50%\"></center>\n",
    "\n",
    "\"Montre-moi qui sont tes amis et je te dirai qui tu es ?\"\n",
    "\n",
    "Le concept du classificateur k-plus proche voisin (k-Nearest Neighbor) peut difficilement être décrit plus simplement. C'est un vieux dicton, que l'on retrouve dans de nombreuses langues et de nombreuses cultures.\n",
    "\n",
    "Cela signifie que le concept du classificateur ```k-plus proche voisin``` fait partie de notre vie quotidienne et de nos jugements : Imaginez que vous rencontriez un groupe de personnes, elles sont toutes très jeunes, élégantes et sportives. Ils parlent de leur ami Ben, qui n'est pas avec eux. Alors, comment imaginez-vous Ben ? Bien, vous l'imaginez jeune, élégant et sportif aussi.\n",
    "\n",
    "Si vous apprenez que Ben vit dans un quartier où les gens votent conservateur et où le revenu moyen est supérieur à 200 000 dollars par an ? Ses deux voisins gagnent même plus de 300 000 dollars par an ? Que pensez-vous de Ben ? Très probablement, vous ne le considérez pas comme un outsider et vous le soupçonnez également d'être conservateur ?\n",
    "\n",
    "Le principe de la classification par les plus proches voisins consiste à trouver un nombre prédéfini ```k```, d'échantillons d'apprentissage les plus proches en distance d'un nouvel échantillon, qui doit être classé. L'étiquette du nouvel échantillon sera définie à partir de ces voisins. Les classificateurs à k voisins les plus proches ont une constante fixe définie par l'utilisateur pour le nombre de voisins qui doivent être déterminés. Il existe également des algorithmes d'apprentissage des voisins basés sur le rayon, dont le nombre de voisins varie en fonction de la densité locale des points, tous les échantillons étant situés dans un rayon fixe. La distance peut, en général, être une mesure métrique quelconque : la distance euclidienne standard est le choix le plus courant. Les méthodes basées sur les voisins sont connues comme des méthodes d'apprentissage automatique non généralisatrices, car elles se \"souviennent\" simplement de toutes leurs données d'apprentissage. La classification peut être calculée par un vote majoritaire des plus proches voisins de l'échantillon inconnu.\n",
    "\n",
    "L'algorithme k-NN est l'un des plus simples de tous les algorithmes d'apprentissage automatique, mais malgré sa simplicité, il s'est avéré très efficace dans un grand nombre de problèmes de classification et de régression, par exemple la reconnaissance de caractères ou l'analyse d'images.\n",
    "\n",
    "Maintenant, allons un peu plus loin sur le plan mathématique :\n",
    "\n",
    "Comme expliqué dans le chapitre Préparation des données, nous avons besoin de données d'apprentissage et de test étiquetées. Contrairement aux autres classificateurs, les classificateurs purs de type \"nearest-neighbor\" n'effectuent aucun apprentissage, mais l'ensemble dit d'apprentissage $LS$ est un composant de base du classificateur. Le classificateur ```kNN``` travaille directement sur les échantillons appris, au lieu de créer des règles, contrairement aux autres méthodes de classification. \n",
    "\n",
    "### Algorithme du plus proche voisin :\n",
    "\n",
    "Étant donné un ensemble de catégories $C=\\{c_1,c_2,\\cdots , c_m\\}$ également appelés classes, par exemple {\"homme\", \"femme\"}. Il existe également un ensemble d'apprentissage $LS$ composé d'instances étiquetées :\n",
    "\n",
    "$$LS=\\{(o_1, c_{o_1}),(o_2, c_{o_2}), \\cdots,(c_n, o_{o_n})\\}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd08c42-fe9f-4a8f-9e7d-2e34696d6ab1",
   "metadata": {},
   "source": [
    "Comme cela n'a aucun sens d'avoir moins d'éléments étiquetés que de catégories, nous pouvons postuler que $n>m$ et dans la plupart des cas, même $n \\gg m$. La tâche de classification consiste à attribuer une catégorie ou classe $c$ à une instance arbitraire $o$.\n",
    "\n",
    "Pour cela, il faut différencier deux cas :\n",
    "\n",
    "- __Cas 1__ : L'instance $o$ est un élément de $LS$ c'est-à-dire qu'il existe un tuple $(o,c)\\in LS$. Dans ce cas, nous utiliserons la classe $c$ comme résultat de la classification.\n",
    "- __Cas 2__ :Nous supposons maintenant que $o$ n'est pas dans $LS$ ou pour être précis :\n",
    "$$\\forall c \\in C\\text{, }  (o, c)\\notin LS$$\n",
    "\n",
    "\n",
    "$o$ est comparée à toutes les instances de $LS$. Une métrique de distance $d$ est utilisée pour les comparaisons.\n",
    "\n",
    "On détermine les $k$ plus proches voisins de $o$ c'est-à-dire les éléments présentant les distances les plus faibles.\n",
    "\n",
    "$k$ est une constante définie par l'utilisateur et un nombre entier positif, qui est généralement petit.\n",
    "\n",
    "Le nombre $k$ est généralement choisi comme la racine carrée de $LS$ le nombre total de points dans l'ensemble de données d'apprentissage.\n",
    "\n",
    "Pour déterminer la valeur de $k$ plus proches voisins, nous réordonnons $LS$ de la manière suivante :\n",
    "\n",
    "$$(o_{i_1}, c_{o_{i_1}}),(o_{i_2}, c_{o_{i_2}}), \\cdots,(o_{i_n}, c_{o_{i_n}})$$\n",
    "\n",
    "de telle sorte que l'expression $d(o_{i_j},o) < d(o_{i_{j+1}},o)$ vraie $\\forall j \\text{ tq }1\\le j \\le n-1$\n",
    "\n",
    "L'ensemble des $k$ plus proches voisins $N_k$ est constitué des $k$ éléments de cet ordre, c'est-à-dire:\n",
    "$$N_k=(o_{i_1}, c_{o_{i_1}}),(o_{i_2}, c_{o_{i_2}}), \\cdots,(o_{i_n}, c_{o_{i_n}})$$\n",
    "\n",
    "La classe la plus courante dans cet ensemble de plus proches voisins $N_k$ sera attribuée à l'instance $o$. S'il n'existe pas de classe unique la plus courante, nous en prenons une arbitrairement.\n",
    "\n",
    "Il n'existe pas de méthode générale pour définir une valeur optimale pour $k$. Cette valeur dépend des données. En règle générale, on peut dire que l'augmentation de $k$ réduit le bruit mais rend les frontières moins distinctes.\n",
    "\n",
    "L'algorithme du classificateur k-NN est l'un des plus simples de tous les algorithmes d'apprentissage automatique. k-NN est un type d'apprentissage basé sur les instances, ou apprentissage paresseux. Dans le domaine de l'apprentissage automatique, on entend par apprentissage paresseux une méthode d'apprentissage dans laquelle la généralisation des données d'apprentissage est retardée jusqu'à ce qu'une requête soit adressée au système. D'autre part, nous avons l'apprentissage rapide, où le système généralise généralement les données d'apprentissage avant de recevoir des requêtes. En d'autres termes : La fonction est seulement approximée localement et tous les calculs sont effectués, lorsque la classification réelle est réalisée.\n",
    "\n",
    "L'image suivante montre de manière simple comment fonctionne le classificateur de plus proche voisin. La pièce du puzzle est inconnue. Pour savoir de quel animal il s'agit, nous devons trouver les voisins. Si $k=1$, le seul voisin est un chat et nous supposons dans ce cas que la pièce du puzzle devrait également être un chat. Si $k=4$, les plus proches voisins sont un poulet et trois chats. Dans ce cas également, il sera prudent de supposer que l'objet en question devrait être un chat.\n",
    "\n",
    "<center><img src=\"img/illustration8_2.png\" width=\"50%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f9c7a-2c4d-4d65-b719-139b77533db0",
   "metadata": {},
   "source": [
    "## Les k-plus-voisins à partir de zéro\n",
    "\n",
    "### Préparation du jeu de données\n",
    "Avant de commencer à écrire un classificateur de plus proches voisins, nous devons réfléchir aux données, c'est-à-dire au jeu de données d'apprentissage et au jeu de données de test. Nous allons utiliser le jeu de données \"iris\" fourni par les jeux de données du module sklearn.\n",
    "\n",
    "Le jeu de données consiste en 50 échantillons de chacune des trois espèces d'Iris suivantes\n",
    "\n",
    "- Iris setosa,\n",
    "- Iris virginica et\n",
    "- Iris versicolor.\n",
    "\n",
    "Quatre caractéristiques ont été mesurées pour chaque échantillon : la longueur et la largeur des sépales et des pétales, en centimètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c3e5b0-d4b8-4a2a-9b64-07115eb53c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "labels = iris.target\n",
    "\n",
    "for i in [0, 79, 99, 121]:\n",
    "    print(f\"index: {i:3}, features: {data[i]}, label: {labels[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b2f29e-6e01-430c-8587-59268aae7637",
   "metadata": {},
   "source": [
    "Nous créons un set d'apprentissage à partir des ensembles ci-dessus. Nous utilisons la permutation de np.random pour diviser les données de façon aléatoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8911687c-8e3a-41eb-800f-b7eae56724c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seeding is only necessary for the website\n",
    "#so that the values are always equal:\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(data))\n",
    "\n",
    "n_test_samples = 12     # number of test samples\n",
    "learn_data = data[indices[:-n_test_samples]]\n",
    "learn_labels = labels[indices[:-n_test_samples]]\n",
    "test_data = data[indices[-n_test_samples:]]\n",
    "test_labels = labels[indices[-n_test_samples:]]\n",
    "\n",
    "print(\"The first samples of our learn set:\")\n",
    "print(f\"{'index':7s}{'data':20s}{'label':3s}\")\n",
    "for i in range(5):\n",
    "    print(f\"{i:4d}   {learn_data[i]}   {learn_labels[i]:3}\")\n",
    "\n",
    "print(\"The first samples of our test set:\")\n",
    "print(f\"{'index':7s}{'data':20s}{'label':3s}\")\n",
    "for i in range(5):\n",
    "    print(f\"{i:4d}   {learn_data[i]}   {learn_labels[i]:3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4084b4ab-7c9b-4b85-9b13-23c1f09816c4",
   "metadata": {},
   "source": [
    "Le code suivant est seulement nécessaire pour visualiser les données de notre set d'apprentissage. Nos données consistent en quatre valeurs par élément d'iris, nous allons donc réduire les données à trois valeurs en additionnant la troisième et la quatrième valeur. De cette façon, nous sommes capables de représenter les données dans un espace tridimensionnel :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeb4aa6-91b9-4012-bce0-952a48afecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "colours = (\"r\", \"b\")\n",
    "X = []\n",
    "for iclass in range(3):\n",
    "    X.append([[], [], []])\n",
    "    for i in range(len(learn_data)):\n",
    "        if learn_labels[i] == iclass:\n",
    "            X[iclass][0].append(learn_data[i][0])\n",
    "            X[iclass][1].append(learn_data[i][1])\n",
    "            X[iclass][2].append(sum(learn_data[i][2:]))\n",
    "\n",
    "colours = (\"r\", \"g\", \"y\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for iclass in range(3):\n",
    "       ax.scatter(X[iclass][0], X[iclass][1], X[iclass][2], c=colours[iclass])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dccd2e-8884-4930-a244-c1541021564c",
   "metadata": {},
   "source": [
    "## Métrique de la distance\n",
    "Nous avons déjà mentionné en détail, que nous calculons les distances entre les points de l'échantillon et l'objet à classer. Pour calculer ces distances, nous avons besoin d'une fonction de distance.\n",
    "\n",
    "Dans les problème de dimension n finie, on utilise généralement l'une des trois métriques de distance suivantes :\n",
    "\n",
    "### La distance euclidienne\n",
    "\n",
    "La distance euclidienne entre deux points ```x``` et ```y``` dans le plan ou l'espace tridimensionnel mesure la longueur d'un segment de droite reliant ces deux points. Elle peut être calculée à partir des coordonnées cartésiennes des points à l'aide du théorème de Pythagore, c'est pourquoi on l'appelle aussi parfois la distance de Pythagore. La formule générale est la suivante:\n",
    "$$d(x,y)=\\sqrt{\\sum_{i=1}^n (x_i-y_i)^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defc33e1-5ec1-454a-accb-a18d4066f7ee",
   "metadata": {},
   "source": [
    "### Distance de Manhattan\n",
    "\n",
    "Elle est définie comme la somme des valeurs absolues des différences entre les coordonnées de x et y :\n",
    "$$d(x,y)=\\sum_{i=1}^n\\vert x_i-y_i\\vert$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d452e8b8-baa3-4697-a8d4-9ebca8b46070",
   "metadata": {},
   "source": [
    "### Distance de Minkowski\n",
    "\n",
    "La distance de Minkowski généralise la distance euclidienne et la distance de Manhattan en une seule métrique de distance. Si nous fixons le paramètre $p$ de la formule suivante à $1$, nous obtenons la distance de Manhattan et si nous utilisons la valeur $2$, nous obtenons la distance euclidienne :\n",
    "\n",
    "$$d(x,y)=\\left(\\sum_{i=1}^n (x_i-y_i)^p\\right)^{\\frac{1}{p}}$$\n",
    "\n",
    "Le diagramme suivant visualise la distance euclidienne et la distance de Manhattan :\n",
    "\n",
    "<center><img src=\"img/illustration8_3.png\" width=\"50%\"></center>\n",
    "\n",
    "La ligne bleue illustre la distance d'Euclide entre le point vert et le point rouge. Sinon, vous pouvez aussi vous déplacer sur la ligne orange, verte ou jaune du point vert au point rouge. Les lignes correspondent à la distance de Manhattan. La longueur est égale.\n",
    "\n",
    "## Détermination des voisins\n",
    "Pour déterminer la similarité entre deux instances, nous allons utiliser la distance euclidienne.\n",
    "\n",
    "Nous pouvons calculer la distance euclidienne avec la fonction norm du module ```np.linalg``` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91250034-8e26-4312-8bdb-3a88ecb92ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(instance1, instance2):\n",
    "    \"\"\" Calculates the Eucledian distance between two instances\"\"\" \n",
    "    return np.linalg.norm(np.subtract(instance1, instance2))\n",
    "\n",
    "print(distance([3, 5], [1, 1]))\n",
    "print(distance(learn_data[3], learn_data[44]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d007baa9-0a1e-4736-8851-a611e5b5ada9",
   "metadata": {},
   "source": [
    "La fonction ```get_neighbors``` renvoie une liste de k voisins, qui sont les plus proches de l'instance test_instance :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70626b91-779f-4cb9-b58d-f481ea61166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(training_set, \n",
    "                  labels, \n",
    "                  test_instance, \n",
    "                  k, \n",
    "                  distance):\n",
    "    \"\"\"\n",
    "    get_neighors calculates a list of the k nearest neighbors\n",
    "    of an instance 'test_instance'.\n",
    "    The function returns a list of k 3-tuples.\n",
    "    Each 3-tuples consists of (index, dist, label)\n",
    "    where \n",
    "    index    is the index from the training_set, \n",
    "    dist     is the distance between the test_instance and the \n",
    "             instance training_set[index]\n",
    "    distance is a reference to a function used to calculate the \n",
    "             distances\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    for index in range(len(training_set)):\n",
    "        dist = distance(test_instance, training_set[index])\n",
    "        distances.append((training_set[index], dist, labels[index]))\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    neighbors = distances[:k]\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d799060f-e705-41f2-957d-e8c77e75c497",
   "metadata": {},
   "source": [
    "Nous allons tester la fonction avec nos échantillons d'iris :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00652673-fea6-4681-906e-0c78812e4159",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    neighbors = get_neighbors(learn_data, \n",
    "                              learn_labels, \n",
    "                              test_data[i], \n",
    "                              3, \n",
    "                              distance=distance)\n",
    "    print(\"Index:         \",i,'\\n',\n",
    "          \"Testset Data:  \",test_data[i],'\\n', \n",
    "          \"Testset Label: \",test_labels[i],'\\n', \n",
    "          \"Neighbors:      \",neighbors,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3806f64-aafc-4948-8d8c-e651129bde3c",
   "metadata": {},
   "source": [
    "### Voter pour obtenir un seul résultat\n",
    "Nous allons maintenant écrire une fonction de vote. Cette fonction utilise la classe Counter des collections pour compter la quantité de classes à l'intérieur d'une liste d'instances. Cette liste d'instance sera bien sûr les voisins. La fonction vote retourne la classe la plus commune :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfbdaf3-b0c5-47dc-9346-ff17bffc613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def vote(neighbors):\n",
    "    class_counter = Counter()\n",
    "    for neighbor in neighbors:\n",
    "        class_counter[neighbor[2]] += 1\n",
    "    return class_counter.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3407b1d-d343-46dc-93a8-c557bdc96c40",
   "metadata": {},
   "source": [
    "Nous allons tester 'vote' sur norte dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d6bf41-0bef-440e-869d-b0753400808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_test_samples):\n",
    "    neighbors = get_neighbors(learn_data, \n",
    "                              learn_labels, \n",
    "                              test_data[i], \n",
    "                              3, \n",
    "                              distance=distance)\n",
    "    print(\"index: \", i, \n",
    "          \", result of vote: \", vote(neighbors), \n",
    "          \", label: \", test_labels[i], \n",
    "          \", data: \", test_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6244edd4-7b21-41ef-9145-8093822ea61b",
   "metadata": {},
   "source": [
    "Nous pouvons voir que les prédictions correspondent aux résultats étiquetés, sauf dans le cas de l'élément avec l'indice 8.\n",
    "\n",
    "```vote_prob``` est une fonction comme ```vote``` mais retourne le nom de la classe et la probabilité pour cette classe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82154457-d362-4788-8c23-bf4f3fe936da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_prob(neighbors):\n",
    "    class_counter = Counter()\n",
    "    for neighbor in neighbors:\n",
    "        class_counter[neighbor[2]] += 1\n",
    "    labels, votes = zip(*class_counter.most_common())\n",
    "    winner = class_counter.most_common(1)[0][0]\n",
    "    votes4winner = class_counter.most_common(1)[0][1]\n",
    "    return winner, votes4winner/sum(votes)\n",
    "for i in range(n_test_samples):\n",
    "    neighbors = get_neighbors(learn_data, \n",
    "                              learn_labels, \n",
    "                              test_data[i], \n",
    "                              5, \n",
    "                              distance=distance)\n",
    "    print(\"index: \", i, \n",
    "          \", vote_prob: \", vote_prob(neighbors), \n",
    "          \", label: \", test_labels[i], \n",
    "          \", data: \", test_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d556a8a7-20f8-4148-b8ac-3f596e4fe061",
   "metadata": {},
   "source": [
    "## Le classificateur des plus proches voisins pondérés\n",
    "Nous n'avons examiné que $k$ éléments dans le voisinage d'un objet inconnu __UO__, et avons procédé à un vote majoritaire. L'utilisation du vote majoritaire s'est avérée très efficace dans notre exemple précédent, mais cela ne tenait pas compte du raisonnement suivant : \n",
    "Plus un voisin est éloigné, plus il __dévie__ du résultat __réel__. En d'autres termes, nous pouvons faire plus confiance aux voisins les plus proches qu'aux plus éloignés. Supposons que nous ayons 11 voisins d'un élément inconnu __UO__. Les cinq voisins les plus proches appartiennent à une classe __A__ et les six autres, qui sont plus éloignés, appartiennent à une classe __B__. Quelle classe doit-on attribuer à __UO__? L'approche précédente dit __B__, car nous avons un vote de 6 contre 5 en faveur de __B__. D'un autre côté, les 5 voisins les plus proches sont tous __A__ et cela devrait compter davantage.\n",
    "\n",
    "Pour poursuivre cette stratégie, nous pouvons attribuer des poids aux voisins de la manière suivante : Le voisin le plus proche d'une instance reçoit un poids 1/1 le deuxième plus proche obtient un poids de 1/2 et ensuite jusqu'à 1/k pour le voisin le plus éloigné.\n",
    "\n",
    "Cela signifie que nous utilisons les séries harmoniques comme poids :\n",
    "\n",
    "$$\\sum_{i=1}^{n-1} \\frac{1}{1+i} = 1+\\frac{1}{2} + \\cdots + \\frac{1}{n}$$\n",
    "\n",
    "Nous mettons cela en œuvre dans la fonction suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_harmonic_weights(neighbors, all_results=True):\n",
    "    class_counter = Counter()\n",
    "    number_of_neighbors = len(neighbors)\n",
    "    for index in range(number_of_neighbors):\n",
    "        class_counter[neighbors[index][2]] += 1/(index+1)\n",
    "    labels, votes = zip(*class_counter.most_common())\n",
    "    #print(labels, votes)\n",
    "    winner = class_counter.most_common(1)[0][0]\n",
    "    votes4winner = class_counter.most_common(1)[0][1]\n",
    "    if all_results:\n",
    "        total = sum(class_counter.values(), 0.0)\n",
    "        for key in class_counter:\n",
    "             class_counter[key] /= total\n",
    "        return winner, class_counter.most_common()\n",
    "    else:\n",
    "        return winner, votes4winner / sum(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a9b7bd-641b-4028-be81-4826890a0f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_test_samples):\n",
    "    neighbors = get_neighbors(learn_data, \n",
    "                              learn_labels, \n",
    "                              test_data[i], \n",
    "                              6, \n",
    "                              distance=distance)\n",
    "    print(\"index: \", i, \n",
    "          \", result of vote: \", \n",
    "          vote_harmonic_weights(neighbors,\n",
    "                                all_results=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa07db47",
   "metadata": {},
   "source": [
    "L'approche précédente ne prenait en compte que le classement des voisins en fonction de leur distance. Nous pouvons améliorer le vote en utilisant la distance réelle. Dans ce but, nous allons écrire une nouvelle fonction de vote :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a781e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_distance_weights(neighbors, all_results=True):\n",
    "    class_counter = Counter()\n",
    "    number_of_neighbors = len(neighbors)\n",
    "    for index in range(number_of_neighbors):\n",
    "        dist = neighbors[index][1]\n",
    "        label = neighbors[index][2]\n",
    "        class_counter[label] += 1 / (dist**2 + 1)\n",
    "    labels, votes = zip(*class_counter.most_common())\n",
    "    #print(labels, votes)\n",
    "    winner = class_counter.most_common(1)[0][0]\n",
    "    votes4winner = class_counter.most_common(1)[0][1]\n",
    "    if all_results:\n",
    "        total = sum(class_counter.values(), 0.0)\n",
    "        for key in class_counter:\n",
    "             class_counter[key] /= total\n",
    "        return winner, class_counter.most_common()\n",
    "    else:\n",
    "        return winner, votes4winner / sum(votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4db41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_test_samples):\n",
    "    neighbors = get_neighbors(learn_data, \n",
    "                              learn_labels, \n",
    "                              test_data[i], \n",
    "                              6, \n",
    "                              distance=distance)\n",
    "    print(\"index: \", i, \n",
    "          \", result of vote: \", \n",
    "          vote_distance_weights(neighbors,\n",
    "                                all_results=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a67ec3",
   "metadata": {},
   "source": [
    "## Un autre exemple de classification par les plus proches voisins\n",
    "\n",
    "Nous voulons tester les fonctions précédentes avec un autre ensemble de données très simple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f55703",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [(1, 2, 2), \n",
    "             (-3, -2, 0),\n",
    "             (1, 1, 3), \n",
    "             (-3, -3, -1),\n",
    "             (-3, -2, -0.5),\n",
    "             (0, 0.3, 0.8),\n",
    "             (-0.5, 0.6, 0.7),\n",
    "             (0, 0, 0)\n",
    "            ]\n",
    "\n",
    "labels = ['apple',  'banana', 'apple', \n",
    "          'banana', 'apple', \"orange\",\n",
    "          'orange', 'orange']\n",
    "\n",
    "k = 2\n",
    "for test_instance in [(0, 0, 0), (2, 2, 2), \n",
    "                      (-3, -1, 0), (0, 1, 0.9),\n",
    "                      (1, 1.5, 1.8), (0.9, 0.8, 1.6)]:\n",
    "    neighbors = get_neighbors(train_set, \n",
    "                              labels, \n",
    "                              test_instance, \n",
    "                              k,\n",
    "                              distance=distance)\n",
    "\n",
    "    print(\"vote distance weights: \", \n",
    "          vote_distance_weights(neighbors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba36328",
   "metadata": {},
   "source": [
    "## kNN en linguistique\n",
    "\n",
    "L'exemple suivant provient de la linguistique informatique. Nous montrons comment nous pouvons utiliser un classificateur kNN pour reconnaître les mots mal orthographiés.\n",
    "\n",
    "Nous utilisons un module appelé levenshtein, qui utilise la distance de Levenshtein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42208d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from levenshtein import levenshtein\n",
    "\n",
    "cities = open(\"data/city_names.txt\").readlines()\n",
    "cities = [city.strip() for city in cities]\n",
    "\n",
    "for city in [\"Freiburg\", \"Frieburg\", \"Freiborg\", \n",
    "             \"Hamborg\", \"Sahrluis\"]:\n",
    "    neighbors = get_neighbors(cities, \n",
    "                              cities, \n",
    "                              city, \n",
    "                              2,\n",
    "                              distance=levenshtein)\n",
    "\n",
    "    print(\"vote_distance_weights: \", vote_distance_weights(neighbors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dd734a",
   "metadata": {},
   "source": [
    "Vous vous demandez peut-être pourquoi la ville de Freiburg n'a pas été reconnue. La raison en est que notre fichier de données avec les noms de ville ne contient que \"Freiburg im Breisgau\". Si vous ajoutez \"Freiburg\" comme entrée, elle sera également reconnue.\n",
    "\n",
    "Marvin et James nous présentent notre prochain exemple :\n",
    "\n",
    "<center><img src=\"img/illustration8_4.png\" width=\"40%\"></center>\n",
    "\n",
    "Pouvez-vous aider Marvin et James ?\n",
    "\n",
    "<center><img src=\"img/illustration8_5.png\" width=\"30%\"></center>\n",
    "\n",
    "Vous aurez besoin d'un dictionnaire anglais et d'un classificateur de type k-nearest Neighbor pour résoudre ce problème. Il se situe dans le répertoire ```data``` de ce notebook.\n",
    "\n",
    "\n",
    "Nous utilisons des mots extrêmement mal orthographiés dans l'exemple suivant. Nous constatons que notre simple fonction vote_prob ne donne de bons résultats que dans deux cas : En corrigeant \"holpposs\" en \"helpless\" et \"blagrufoo\" en \"barefoot\". Alors que notre vote à distance fonctionne bien dans tous les cas. Bon, nous devons admettre que nous avions \"liberty\" à l'esprit, lorsque nous avons écrit \"liberdi\", mais suggérer \"liberal\" est un bon choix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59c4fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "with open(\"data/british-english.txt\") as fh:\n",
    "    for line in fh:\n",
    "        word = line.strip()\n",
    "        words.append(word)\n",
    "\n",
    "for word in [\"holpful\", \"kundnoss\", \"holpposs\", \"thoes\", \"innerstand\",\n",
    "             \"blagrufoo\", \"liberdi\"]:\n",
    "    neighbors = get_neighbors(words, \n",
    "                              words, \n",
    "                              word, \n",
    "                              3,\n",
    "                              distance=levenshtein)\n",
    "\n",
    "    print(\"vote_distance_weights: \", vote_distance_weights(neighbors, \n",
    "                                                           all_results=False))\n",
    "    print(\"vote_prob: \", vote_prob(neighbors))\n",
    "    print(\"vote_distance_weights: \", vote_distance_weights(neighbors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3eea29",
   "metadata": {},
   "source": [
    "[Suivant](9_knn_sklearn.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
