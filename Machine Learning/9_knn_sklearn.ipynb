{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b799822",
   "metadata": {},
   "source": [
    "# 9. Classificateur k-plus proche-voisin avec sklearn\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "Les concepts sous-jacents du classificateur K-plus-proches-voisins (kNN) peuvent être trouvés dans le chapitre Classificateur K-plus-proches-voisins de notre Tutoriel sur l'apprentissage automatique. Dans ce chapitre, nous avons également présenté des fonctions simples écrites en Python pour démontrer les principes fondamentaux.\n",
    "\n",
    "Au lieu d'utiliser ces fonctions, même si elles donnent des résultats impressionnants, nous vous recommandons d'utiliser les fonctionnalités du module sklearn. Nous avons déjà utilisé sklearn dans nos chapitres précédents.\n",
    "\n",
    "### Utilisation de sklearn pour kNN\n",
    "\n",
    "```neighbors``` est un paquetage du module ```sklearn```, qui fournit des fonctionnalités pour les classificateurs du plus proche voisin, à la fois pour l'apprentissage supervisé et non supervisé.\n",
    "\n",
    "Les classes de ```sklearn.neighbors``` peuvent traiter à la fois des tableaux ```Numpy``` et des matrices ```scipy.sparse``` en entrée. Pour les matrices denses, un grand nombre de mesures de distance possibles sont supportées. Pour les matrices éparses, des métriques de Minkowski arbitraires sont supportées pour les recherches.\n",
    "\n",
    "```scikit-learn``` implémente deux classificateurs plus proches voisins différents :\n",
    "\n",
    "- __KNeighborsClassifier__ est basé sur les $k$ plus proches voisins d'un échantillon, qui doit être classé. Le nombre $k$ est une valeur entière spécifiée par l'utilisateur. Il s'agit du classificateur le plus fréquemment utilisé par les deux algorithmes.\n",
    "- __RadiusNeighborsClassifier__ est basé sur le nombre de voisins dans un rayon fixe $r$ pour chaque échantillon qui doit être classifié. $r$ est une valeur flottante spécifiée par l'utilisateur. Ce classificateur est moins souvent utilisé.\n",
    "\n",
    "### KNeighborsClassifier\n",
    "Nous allons créer artificiellement un jeu de données avec trois classes pour tester le classificateur kNN ```KNeighborsClassifier``` de ```sklearn.neighbors```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57009967",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "centers = [[2, 3], [5, 5], [1, 8]]\n",
    "n_classes = len(centers)\n",
    "data, labels = make_blobs(n_samples=150, \n",
    "                          centers=np.array(centers),\n",
    "                          random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ba3dcf",
   "metadata": {},
   "source": [
    "Visualisons ce que nous avons créé :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10cad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colours = ('green', 'red', 'blue')\n",
    "n_classes = 3\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for n_class in range(0, n_classes):\n",
    "    ax.scatter(data[labels==n_class, 0], data[labels==n_class, 1], \n",
    "               c=colours[n_class], s=10, label=str(n_class))\n",
    "\n",
    "\n",
    "\n",
    "ax.legend(loc='upper right');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc5bb25",
   "metadata": {},
   "source": [
    "Nous devons maintenant diviser les données en un ensemble de test et d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde2ab35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "res = train_test_split(data, labels, \n",
    "                       train_size=0.8,\n",
    "                       test_size=0.2,\n",
    "                       random_state=1)\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = res "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7680f3",
   "metadata": {},
   "source": [
    "Nous sommes maintenant prêts à effectuer la classification avec ```kNeighborsClassifier``` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc85d698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit a nearest-neighbor classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_data, train_labels) \n",
    "\n",
    "predicted = knn.predict(test_data)\n",
    "print(\"Predictions from the classifier:\")\n",
    "print(predicted)\n",
    "print(\"Target values:\")\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1816af71",
   "metadata": {},
   "source": [
    "Pour évaluer le résultat, nous utiliserons ```accuracy_score``` du module ```sklearn.metrics```. Pour voir comment ```accuracy_score``` fonctionne, nous allons utiliser un exemple simple avec des pseudo prédictions et des étiquettes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c67569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "example_predictions = [0, 2, 1, 3, 2, 0, 1]\n",
    "example_labels      = [0, 1, 2, 3, 2, 1, 1]\n",
    "print(accuracy_score(example_predictions, example_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cfb7ea",
   "metadata": {},
   "source": [
    "La valeur de retour correspond au quotient des éléments correctement classés et du nombre total de prédictions. Si vous êtes uniquement intéressé par le nombre d'éléments correctement classés, vous pouvez définir le paramètre normalize sur ```False```. La valeur par défaut est ```True```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e9de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(example_predictions, \n",
    "                     example_labels,\n",
    "                     normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82afb5bc",
   "metadata": {},
   "source": [
    "Nous sommes maintenant prêts à évaluer les résultats de notre précédent exemple de clissification :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fde238",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(predicted, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc870a0b",
   "metadata": {},
   "source": [
    "Vous avez peut-être remarqué que nous avons instancié le classificateur de kNN dans notre exemple précédent en l'appelant sans aucun argument, c'est-à-dire ```KNeighborsClassifier()```. Dans ce qui suit, nous l'instancions avec quelques paramètres de mots-clés possibles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496f09aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(algorithm='auto', \n",
    "                           leaf_size=30, \n",
    "                           metric='minkowski',\n",
    "                           p=2,\n",
    "                           metric_params=None, \n",
    "                           n_jobs=1, \n",
    "                           n_neighbors=5, \n",
    "                           weights='uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c060da74",
   "metadata": {},
   "source": [
    "Le paramètre métrique est Minkowski par défaut. Le paramètre p correspond à l'ordre de la distance dans la formule de Minkowski : Lorsque $p$ est défini à 1, cela équivaut à utiliser la __distance de manhattan__. Quand $p=2$ on utilise la __distance euclidienne__.\n",
    "\n",
    "Le paramètre ```algorithm``` détermine quel algorithme sera utilisé, par exemple\n",
    "\n",
    "- ball_tree utilisera BallTree\n",
    "- kd_tree utilisera KDTree\n",
    "- brute utilisera une recherche par force brute. \n",
    "\n",
    "En laissant le paramètre sur auto le classifieur détermine l'algorithme le plus approprié en fonction des valeurs passées à la méthode ```fit```.\n",
    "Le paramètre ```leaf_size``` est nécessaire pour ```BallTree``` ou ```KDTree```. Il peut affecter la vitesse de construction et d'interrogation, ainsi que la mémoire requise pour stocker l'arbre. La valeur optimale dépend de la nature du problème.\n",
    "\n",
    "### Utilisation des données Iris\n",
    "\n",
    "Dans l'exemple suivant, nous allons utiliser l'ensemble de données Iris :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cd19b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "data, labels = iris.data, iris.target\n",
    "\n",
    "res = train_test_split(data, labels, \n",
    "                       train_size=0.8,\n",
    "                       test_size=0.2,\n",
    "                       random_state=12)\n",
    "train_data, test_data, train_labels, test_labels = res \n",
    "# Create and fit a nearest-neighbor classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# classifier \"out of the box\", no parameters\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_data, train_labels) \n",
    "\n",
    "\n",
    "print(\"Predictions from the classifier:\")\n",
    "test_data_predicted = knn.predict(test_data)\n",
    "print(test_data_predicted)\n",
    "print(\"Target values:\")\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d3bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(test_data_predicted, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd97363",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predictions from the classifier:\")\n",
    "learn_data_predicted = knn.predict(train_data)\n",
    "print(learn_data_predicted)\n",
    "print(\"Target values:\")\n",
    "print(train_labels)\n",
    "print(accuracy_score(learn_data_predicted, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8cbb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2 = KNeighborsClassifier(algorithm='auto', \n",
    "                            leaf_size=30, \n",
    "                            metric='minkowski',\n",
    "                            p=2,         # p=2 is equivalent to euclidian distance\n",
    "                            metric_params=None, \n",
    "                            n_jobs=1, \n",
    "                            n_neighbors=5, \n",
    "                            weights='uniform')\n",
    "\n",
    "knn.fit(train_data, train_labels) \n",
    "test_data_predicted = knn.predict(test_data)\n",
    "accuracy_score(test_data_predicted, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb49e66",
   "metadata": {},
   "source": [
    "### RadiusNeighborsClassifier\n",
    "Le mode de fonctionnement du classificateur à k plus proches voisins consiste à agrandir un cercle autour de l'échantillon inconnu (c'est-à-dire l'élément qui doit être classé) jusqu'à ce que le cercle contienne exactement k éléments. Le classificateur Radius Neighbors a une longueur fixe pour le cercle qui l'entoure. Il localise tous les éléments de l'ensemble de données d'apprentissage qui se trouvent dans le cercle avec la longueur de rayon donnée autour de l'élément qui doit être classé. En conséquence de l'approche à rayon fixe, les régions denses de la distribution des caractéristiques fourniront plus d'informations et les régions clairsemées en fourniront moins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2080fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "\n",
    "X = [[0, 1], [0.5, 1], [3, 1], [3, 2], [1.3, 0.8], [2.5, 2.5], [2.4, 2.6]]\n",
    "y = [0, 0, 1, 1, 0, 1, 1]\n",
    "\n",
    "neigh = RadiusNeighborsClassifier(radius=1.0)\n",
    "neigh.fit(X, y)\n",
    "\n",
    "print(neigh.predict([[1.5, 1.2]]))\n",
    "\n",
    "print(neigh.predict([[3.1, 2.1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbc73be",
   "metadata": {},
   "source": [
    "Si nous essayons de faire une prédiction sur des données comme ```[30, 20]```, l'algorithme ne peut pas trouver de voisins pour le rayon 1.0. Il lèvera donc une exception avec le texte suivant :\n",
    "\n",
    "```shell\n",
    "ValueError: No neighbors found for test samples array([0]), you can try using larger radius, giving a label for outliers, or considering removing them from your dataset.\n",
    "```\n",
    "\n",
    "Il existe un paramètre permettant de définir l'étiquette pour les valeurs aberrantes, à savoir outlier_label.\n",
    "\n",
    "Il y a trois façons de l'utiliser :\n",
    "\n",
    "1. étiquette manuelle : étiquette str ou int (devrait être le même type que celui utilisé dans nos données) ou liste d'étiquettes manuelles si une sortie multiple est utilisée.\n",
    "2. Il peut être défini avec la valeur ```most_frequent```. Ceci attribuera aux valeurs aberrantes l'étiquette la plus fréquente de l'ensemble de données.\n",
    "3. S'il est défini sur None (par défaut), une ValueError sera générée lorsqu'une valeur aberrante sera détectée.\n",
    "\n",
    "\n",
    "Recommençons avec ```most_frequent```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca44428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = RadiusNeighborsClassifier(radius=1.0,\n",
    "                                  outlier_label='most_frequent')\n",
    "neigh.fit(X, y)\n",
    "\n",
    "print(neigh.predict([[1.5, 1.2]]))\n",
    "\n",
    "# the following is the previously mentioned outlier:\n",
    "print(neigh.predict([[30, 20]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0e1df8",
   "metadata": {},
   "source": [
    "Alternativement, nous définissons la classe aberrante à 2. Nous ajoutons un élément aberrant à notre learnset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b93dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "\n",
    "X = [[0, 1], [0.5, 1], [3, 1], [3, 2], [1.3, 0.8], [2.5, 2.5], [2.4, 2.6], [10000, -2321]]\n",
    "y = [0, 0, 1, 1, 0, 1, 1, 2]\n",
    "\n",
    "neigh = RadiusNeighborsClassifier(radius=1.0,\n",
    "                                  outlier_label=2)\n",
    "neigh.fit(X, y)\n",
    "\n",
    "print(neigh.predict([[1.5, 1.2]]))\n",
    "print(neigh.predict([[30, 20]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6af72c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "centers = [[2, 3], [9, 2], [7, 9]]\n",
    "n_classes = len(centers)\n",
    "data, labels = make_blobs(n_samples=255, \n",
    "                          centers=np.array(centers),\n",
    "                          cluster_std = 1.3,\n",
    "                          random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60354eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dbab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colours = ('green', 'red', 'blue')\n",
    "n_classes = 3    # not using the outlier 'class'\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for n_class in range(0, n_classes):\n",
    "    ax.scatter(data[labels==n_class, 0], data[labels==n_class, 1], \n",
    "               c=colours[n_class], s=10, label=str(n_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec688bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = train_test_split(data, labels, \n",
    "                       train_size=0.8,\n",
    "                       test_size=0.2,\n",
    "                       random_state=1)\n",
    "train_data, test_data, train_labels, test_labels = res "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac49912",
   "metadata": {},
   "source": [
    "Ajoutons une ligne à la fin de ```train_data``` qui contient des données aberrantes, c'est-à-dire n'appartenant à aucune classe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef427ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier = [4242.2, 4242.2]\n",
    "train_data = np.vstack([train_data, outlier])\n",
    "train_data[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e774a11",
   "metadata": {},
   "source": [
    "Maintenant, nous devons ajouter une étiquette aberrante aux étiquettes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1ae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_label = len(np.unique(labels))\n",
    "train_labels = np.append(train_labels, outlier_label)\n",
    "train_labels[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bbf21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96f446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RadiusNeighborsClassifier(radius=1)\n",
    "rnn.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5562098",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = rnn.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512216e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(predicted, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a457a",
   "metadata": {},
   "source": [
    "Réduisons le rayon :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5be89cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RadiusNeighborsClassifier(radius=0.9,\n",
    "                                outlier_label=outlier_label)\n",
    "rnn.fit(train_data, train_labels)\n",
    "predicted = rnn.predict(test_data)\n",
    "print(accuracy_score(predicted, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383aadd6",
   "metadata": {},
   "source": [
    "Créons quelques valeurs aberrantes et testons-les :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d1a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = [[100, 300]]\n",
    "data_outliers, labels_outliers = make_blobs(n_samples=10, \n",
    "                                  centers=np.array(centers),\n",
    "                                  random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6afcd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = rnn.predict(data_outliers)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b5461",
   "metadata": {},
   "source": [
    "Une bonne valeur pour k est la racine carrée de tous les échantillons de l'ensemble d'apprentissage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd8e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = int(len(labels) ** 0.5)\n",
    "# make this value odd:\n",
    "if k % 2 == 0:\n",
    "    k += 1\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610baab7",
   "metadata": {},
   "source": [
    "Comparons cela avec un classificateur à k plus proches voisins :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3d30c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(algorithm='auto', \n",
    "                     leaf_size=30, \n",
    "                     metric='minkowski',\n",
    "                     metric_params=None, \n",
    "                     n_jobs=1, \n",
    "                     n_neighbors=k, # default is 5\n",
    "                     p=2,         # p=2 is equivalent to euclidian distance\n",
    "                     weights='uniform')\n",
    "\n",
    "knn.fit(data, labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0912c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = knn.predict(test_data)\n",
    "print(accuracy_score(predicted, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e47f720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "# Evaluate Model\n",
    "cm = confusion_matrix(predicted, test_labels)\n",
    "print(cm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd29737",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = knn.predict(data_outliers)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01877613",
   "metadata": {},
   "source": [
    "Nous pouvons voir que toutes les valeurs aberrantes ont été classées à tort dans la classe 2, car c'est la classe existante la plus proche des valeurs aberrantes. Nous créons dans la suite trois clusters de valeurs aberrantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7094ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = [[100, 300], [10, -10], [-200, -200]]\n",
    "data_outliers2, labels_outliers2 = make_blobs(n_samples=30, \n",
    "                                              centers=np.array(centers),\n",
    "                                              random_state=1)\n",
    "\n",
    "predicted = knn.predict(data_outliers2)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0386346e",
   "metadata": {},
   "source": [
    "Les valeurs aberrantes sont assignées aux clusters existants même si elles en sont éloignées. D'autre part, le ```RadiusNeighbirClassifier``` les reconnaîtra comme des valeurs aberrantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b19475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RadiusNeighborsClassifier(radius=0.9,\n",
    "                                outlier_label=outlier_label)\n",
    "rnn.fit(train_data, train_labels)\n",
    "predicted = rnn.predict(data_outliers2)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f26f8e",
   "metadata": {},
   "source": [
    "## Exercices\n",
    "\n",
    "### Exercice 1\n",
    "Classifiez les données de ```strange_flowers.txt``` avec un classificateur à k plus proches voisins.\n",
    "\n",
    "___Solution___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c08113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler # necessary to reduce biases of large numbers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "dataset = pd.read_csv(\"data/strange_flowers.txt\", \n",
    "                      header=None, \n",
    "                      names=[\"red\", \"green\", \"blue\", \"size\", \"label\"],\n",
    "                      sep=\" \")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad40a8a8",
   "metadata": {},
   "source": [
    "Au lieu d'utiliser Pandas pour lire les données de ```strange_flowers.txt```, nous pourrions utiliser ```loadtxt``` de numpy :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10551936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative way to read and extract the data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "raw_data = np.loadtxt(\"data/strange_flowers.txt\")\n",
    "data = raw_data[:,:-1]\n",
    "labels = raw_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede6aa70",
   "metadata": {},
   "source": [
    "Nous allons maintenant poursuivre avec l'objet Pandas DataFrame ```dataset```, que nous lisons avec ```read_csv``` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af538ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.drop('label', axis=1)\n",
    "labels = dataset.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6de241",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, \n",
    "                                                    labels, \n",
    "                                                    random_state=0, \n",
    "                                                    test_size=0.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778730d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() \n",
    "X_train = scaler.fit_transform(X_train) #  transform\n",
    "X_test = scaler.transform(X_test) #  transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f61dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a60e831",
   "metadata": {},
   "source": [
    "Nous fixons k à la racine carrée de la taille de l'ensemble d'apprentissage :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472747e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = int(len(X_train) ** 0.5)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7565225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "classifier = KNeighborsClassifier(n_neighbors=k, \n",
    "                                  metric=\"minkowski\",\n",
    "                                  p=2,    # Euclidian\n",
    "                                 ) #  p for different label types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e9807",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b64489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550f71b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d9996c",
   "metadata": {},
   "source": [
    "## Détermination de la valeur optimale de k\n",
    "\n",
    "Comme nous l'avons écrit, la valeur optimale de k est généralement la racine carrée de n, où n est le nombre total d'échantillons de notre ensemble de données.\n",
    "\n",
    "Nous pouvons également déterminer une valeur pour k en traçant les valeurs de précision pour différentes valeurs de k :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "n_classes = 6\n",
    "data, labels = make_blobs(n_samples=1000, \n",
    "                          centers=n_classes,\n",
    "                          cluster_std = 1.3,\n",
    "                          random_state=1)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colours = ('green', 'red', 'blue', 'magenta', 'yellow', 'pink')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for n_class in range(0, n_classes):\n",
    "    ax.scatter(data[labels==n_class, 0], data[labels==n_class, 1], \n",
    "               c=colours[n_class], s=10, label=str(n_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a791a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = train_test_split(data, labels, \n",
    "                       train_size=0.7,\n",
    "                       test_size=0.3,\n",
    "                       random_state=1)\n",
    "train_data, test_data, train_labels, test_labels = res \n",
    "\n",
    "print(len(train_data), len(test_data), len(train_labels))\n",
    "\n",
    "X, Y = [], []\n",
    "for k in range(1, 25):\n",
    "    classifier = KNeighborsClassifier(n_neighbors=k, \n",
    "                                      p=2,    # Euclidian\n",
    "                                      metric=\"minkowski\")\n",
    "    classifier.fit(train_data, train_labels)\n",
    "    predictions = classifier.predict(test_data)\n",
    "    score = accuracy_score(test_labels, predictions)\n",
    "    X.append(k)\n",
    "    Y.append(score)\n",
    "    \n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('accuracy')\n",
    "ax.plot(X, Y, \"go\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac65ab6b",
   "metadata": {},
   "source": [
    "[Suivant](10_introduction_reseaux_neurones.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
